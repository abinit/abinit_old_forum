<!DOCTYPE html>
<html dir="ltr" lang="en-gb">

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=9&t=2124&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 03:23:17 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="robots" content="noindex" />

<title>ABINIT Discussion Forums &bull; Scaling up - mpirun, mkmem, k-points</title>

<link href="styles/flat-style/theme/print.css" rel="stylesheet">
<link href="styles/flat-style/theme/bidi.css" rel="stylesheet">
</head>
<body id="phpbb" class="ltr">
<div id="wrap" class="wrap">
	<a id="top" class="top-anchor" accesskey="t"></a>

	<div id="page-header">
		<h1>ABINIT Discussion Forums</h1>
		<p>The meeting place for ABINIT users and developers<br /><a href="index.html">https://forum.abinit.org/</a></p>

		<h2>Scaling up - mpirun, mkmem, k-points</h2>
		<p><a href="viewtopic1794.html?f=9&amp;t=2124">https://forum.abinit.org/viewtopic.php?f=9&amp;t=2124</a></p>
	</div>

	<div id="page-body" class="page-body">
		<div class="page-number">Page <strong>1</strong> of <strong>1</strong></div>
					<div class="post">
				<h3>Scaling up - mpirun, mkmem, k-points</h3>
				<div class="date">Posted: <strong>Wed May 08, 2013 1:59 am</strong></div>
				<div class="author">by <strong>paduano</strong></div>
				<div class="content">hi,<br /><br />   I am working on a NV-diamond problem and I am trying to scale up<br />   the number of k-points in my mesh to resolve fine structure in DoS. <br />   I am encountering two unexpected problems:<br /><br />     1.  When I increase the number of k-points, I increase the memory<br />          footprint of a single processor job.  This was unexpected; I<br />          thought the solution of the problem at each k-point was<br />          independent of the other k-points, that Etotal was an average over<br />          k-points (via wtk) of the occupied bands/states.   <br /><br />          I see from reading docs for mkmem that by default all nkpt ground <br />          state wavefn's are kept in memory.  Why is more than one at a time<br />          needed?  When I change this setting to 0, it impacts memory footprint<br />          dramatically, but not run time.  Why is the default nkpt and not 0?<br /><br /><br />     2.  When I use &quot;mpirun -np X&quot; and try to take advantage of cores on a single<br />          box, I cannot come close to the ratios from the plot in the tutorial.   I <br />          see small speedup from X=1 to X=2 (1.5-1.7) and almost no speedup from <br />          X=2 to X=4.   There is implied overhead scaling up with X; I expected the<br />          work to be (much) more independent and the plots in the tutorial suggest<br />          this should be so.  Why might my jobs be experiencing so much more <br />          overhead than expected?  My system is 99.9% idle except for these jobs.<br /><br /><br />matt</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Scaling up - mpirun, mkmem, k-points</h3>
				<div class="date">Posted: <strong>Sat May 11, 2013 3:53 pm</strong></div>
				<div class="author">by <strong>gmatteo</strong></div>
				<div class="content"><blockquote class="uncited"><div>1. When I increase the number of k-points, I increase the memory<br />footprint of a single processor job. This was unexpected; I<br />thought the solution of the problem at each k-point was<br />independent of the other k-points, that Etotal was an average over<br />k-points (via wtk) of the occupied bands/states. <br /></div></blockquote><br /><br />Why unexpected? All the k-points are stored in memory during the SCF cycle to avoid IO that would<br />be detrimental for the efficiency.<br />The MPI version distributes the k-points among the processors (also bands and G if paral_kgb is used),<br />hence the memory scales with the number of MPI nodes (keeping the size of the problem fixed).<br />Obviously the memory footprint increases If you increase the number of k-points while keeping the number of MPI nodes constant.<br /><br /><br /><blockquote class="uncited"><div>Why is more than one at a time needed? When I change this setting to 0, it impacts memory footprint<br />dramatically, but not run time. Why is the default nkpt and not 0?<br /></div></blockquote><br /><br />We do that to avoid having to read the block of wavefunctions  at each step of the SCF cycle.<br />Sorry, but I cannot believe that mkem=0 does not have any impact on the run-time. Could you provide the input file of the run?<br /><br /><br /><blockquote class="uncited"><div>2. When I use &quot;mpirun -np X&quot; and try to take advantage of cores on a single<br />box, I cannot come close to the ratios from the plot in the tutorial. I <br />see small speedup from X=1 to X=2 (1.5-1.7) and almost no speedup from <br />X=2 to X=4.<br /></div></blockquote><br /><br />How many (physical) cores do you have in your machine?  Configuration details?</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Scaling up - mpirun, mkmem, k-points</h3>
				<div class="date">Posted: <strong>Tue May 14, 2013 11:51 pm</strong></div>
				<div class="author">by <strong>paduano</strong></div>
				<div class="content">hi,<br /><br />  Thanks for your reply.<br /><br />  One point that wasn't clear from my original post:  I happen to be <br />  computing the bands for a given input density file (e.g. iscf=-2)<br />  because I am trying to scale nkpt.<br /><br />  For true SCF calcs, I see why one needs to know the (new) total density <br />  at each iteration before continuing for a given k-point.  I am not sure<br />  why the *density* would be different from k-point to k-point in a given<br />  band.    But I suppose it could be (I expect they are are least very similar<br />  in practice)...<br /><br />  &gt;  Why unexpected?<br /><br />  Because I can compute the bands/spectra one k-point at a time and I get<br />  exactly the same band structure in exactly the same total CPU time (at <br />  least with iscf=-2).  Yet, either in memory or on disk, abinit tracks a large<br />  WF structure that grows with nkpt (i.e. mkmem).  I can beat this by asking <br />  abinit to compute one k-point at a time and then recombine the bands <br />  myself at the end.  Otherwise, I'd never be able to set nkpt to, say, 50,000.<br />  <br />  So I wonder why abinit seems to make me do it this way.  I've done<br />  example calcs for the 63 atom NV diamond defect and the results are<br />  *exactly* the same whether I run abinit once with nkpt=1000 or whether <br />  I break those 1000 kpts into subsets and recombine myself...   I've tried<br />  various ways of choosing the subsets, both in terms of size and in terms<br />  of BZ sampling.  All lead to same DoS/bands... I haven't tried this with<br />  a full SCF calc.<br /><br />  [ an annoying aside related to this manual procedure is that abinit won't <br />  read in the wtk vector when I use kptopt=0.  abinit writes out the EIG file<br />  with all the wtk values as 1.0 and I have to track the weights separately.<br />  This fact alone, that the results are independent of wtk, is another way of<br />  saying that these calcs are independent from k-point to k-point.  Hence, <br />  why drag around some big structure and not just compute them one at a<br />  time and move on? ] <br /><br /><br />&gt;  cannot believe that mkem=0 does not have any impact on the run-time.<br /><br />  you are quite correct!  The small calc I did when writing the post had a <br />  small nkpt and that file appears to have been efficiently cached by the<br />  disk cache in the kernel on my linux box.   When I let the file grow<br />  beyond size of main memory, things gum up rapidly, as expected.<br /><br />  I agree that the default value of mkmem makes sense as one wants to <br />  know when setting the &quot;go really really slow&quot; flag.<br /><br /><br />&gt; How many (physical) cores do you have in your machine? Configuration details?<br /><br />  I think I've mostly figured this part out.  On my laptop (intel core i5), it appears<br />  that the core pipelines are not fully independent for abinit workload.   For two<br />  parallel jobs, I get a 1.7x speedup.  For four parallel jobs, I get something like 1.9x <br />  (in 4x the CPU time!).   I'm not sure what instructions/processing-units are blocking<br />  my i5, but when I run the same jobs sequentially the CPU times add up.   I.e., <br />  with a cluster of independent machines or with a better CPU (e.g. Xeon) with<br />  fully redundant cores I see 2x, 4x etc.   I have seen this with other code I write,<br />  but only with 4 jobs, never with just 2 before... I thought the i5 had at least <br />  2 independent cores that were each hyper-threaded, but I now have proof otherwise.<br />  One must pay some mind to all the marketing BS from Intel!<br /><br /><br />matt</div>
			</div>
			<hr />
			</div>

	<div id="page-footer" class="page-footer">
		<div class="page-number">All times are <span title="Europe/Brussels">UTC+02:00</span><br />Page <strong>1</strong> of <strong>1</strong></div>
			<div class="copyright">
				<p>Powered by <a href="https://www.phpbb.com/">phpBB</a>&reg; Forum Software &copy; phpBB Limited
				</p>
							</div>
	</div>
</div>

</body>

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=9&t=2124&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 03:23:17 GMT -->
</html>
