<!DOCTYPE html>
<html dir="ltr" lang="en-gb">

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=3&t=3801&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 20 Sep 2024 23:12:25 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="robots" content="noindex" />

<title>ABINIT Discussion Forums &bull; defective inter-node parallelism with 2017 Intel compilers</title>

<link href="styles/flat-style/theme/print.css" rel="stylesheet">
<link href="styles/flat-style/theme/bidi.css" rel="stylesheet">
</head>
<body id="phpbb" class="ltr">
<div id="wrap" class="wrap">
	<a id="top" class="top-anchor" accesskey="t"></a>

	<div id="page-header">
		<h1>ABINIT Discussion Forums</h1>
		<p>The meeting place for ABINIT users and developers<br /><a href="index.html">https://forum.abinit.org/</a></p>

		<h2>defective inter-node parallelism with 2017 Intel compilers</h2>
		<p><a href="viewtopic19fb.html?f=3&amp;t=3801">https://forum.abinit.org/viewtopic.php?f=3&amp;t=3801</a></p>
	</div>

	<div id="page-body" class="page-body">
		<div class="page-number">Page <strong>1</strong> of <strong>1</strong></div>
					<div class="post">
				<h3>defective inter-node parallelism with 2017 Intel compilers</h3>
				<div class="date">Posted: <strong>Tue Mar 20, 2018 1:20 pm</strong></div>
				<div class="author">by <strong>danric</strong></div>
				<div class="content">Hello!<br /><br />After trying a lot to solve this, I thought to ask help on this forum as well. Thank you in advance for your advice.<br /><br />I would like to start using a system that only has the compilers from Intel Composer 2017. Have configured as suggested by the Intel MKL advisor and I can get the abinit executables. The behavior I notice is that abinit calculates OK when running with processors of a single node, but whenever I to use &gt;1 nodes, either the geometry relaxation refuses to converge after a few Broyden steps, crashes after a few Broyden steps, or the calculation completes, but in a longer time than in case of using a single node. So, intra-node parallelism is OK, but inter-node parallelism is not. The behavior is essentially the same with abinit versions 7.8.1, 8.2.2 and 8.6.3.<br /><br />If I compile same versions of abinit using compilers of Intel Composer 2013 (on another platform that accepts both 2013 and 2017 versions) , it is all OK: my test calculations converge and the time reasonably scales with the number of processors used. And the 2017 compilers still don't provide me with executables that run well on more than 1 node on that platform too.<br /><br />It might also be relevant to mention that I always need to do some tricks to allow the ./configure to recognize the Intel compilers. In addition to specifying the MPI prefix, I have to manually replace mpif90-&gt; mpiiforc etc (similar for C and C++) inside the configure file, and also specify with_fc_vendor=&quot;intel&quot;, with_fc_version=&quot;17.0.5&quot; etc (similar for C and C++) in the .ac file. If I don't do the above, either the configuration fails or I must manually copy all the .mod files gradually generated in /src directories in the src/mods or src/incs directory in order for the compilation to succeed. I wonder if some irregular behavior is not caused by the configure script being unable to recognize the compilers by itself as it should... though again, using 2013 compilers it always runs OK.<br /><br />I can provide more details (log files or specific build options used) if someone has an idea about what could be done in order to make inter-node parallelism work with 2017 Intel compilers. <br /><br />I have seen that someone reported some issues when using more than 1 node back in 2012, though that was for 2013 Intel compilers that in my case work fine, and also I don't always get job crashes. Thus the cause is probably different.<br /><br /><!-- l --><a class="postlink-local" href="viewtopicff1d.html?f=3&amp;t=1851">viewtopic.php?f=3&amp;t=1851</a><!-- l --><br /><br />Many thanks for reading and for any clues.<br />Dan</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: defective inter-node parallelism with 2017 Intel compile</h3>
				<div class="date">Posted: <strong>Tue Mar 20, 2018 8:17 pm</strong></div>
				<div class="author">by <strong>ebousquet</strong></div>
				<div class="content">Dear danric,<br />This is probably related to compilation flags you are using. Could you send your config.ac file or the list of config flags you are using?<br />On which architecture/processors are you running on?<br />Best wishes,<br />Eric</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: defective inter-node parallelism with 2017 Intel compile</h3>
				<div class="date">Posted: <strong>Tue Mar 20, 2018 9:35 pm</strong></div>
				<div class="author">by <strong>ebousquet</strong></div>
				<div class="content">This post might help in solving your problem:<br /><a href="viewtopic6d33.html?f=3&amp;t=3391&amp;p=10348#p10348" class="postlink">https://forum.abinit.org/viewtopic.php?f=3&amp;t=3391&amp;p=10348#p10348</a><br />Eric</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: defective inter-node parallelism with 2017 Intel compile</h3>
				<div class="date">Posted: <strong>Wed Mar 21, 2018 5:56 pm</strong></div>
				<div class="author">by <strong>danric</strong></div>
				<div class="content">Thank you, Eric, for your kindness.<br /><br />I was indeed very hopeful to try the enable_avx_safe_mode option as suggested in that post, but unfortunately the behavior is still not right. For abinit v863, it's OK when running on 1 node (18 processors), and refuses to converge while doing even the first relaxation step of my test case, when running with 36 processors on 2 nodes. <br /><br />I run abinit on a Intel Xeon E5-2670v2 cluster and also trying one with Intel Xeon Gold 6126 processors. For first one I can run OK with 2013 Intel compilers, but only have 2017 compilers for the 2nd, which means that I can only use 1 node there. <br /><br />Here are the key options in my .ac file that I usually set. These build options are the result of my reading of internet and experience with compiling abinit (some of these might be overkill, but probably they don't harm). I have also used -O2 instead of -O3, with no improvement in the described behavior.<br /><br />enable_optim=&quot;yes&quot;<br />CFLAGS_OPTIM='-O3 -mtune=native -march=native -xHost'<br />CXXFLAGS_OPTIM='-O3 -mtune=native -march=native -xHost'<br />FCFLAGS_OPTIM='-O3 -mtune=native -march=native -xHost'<br />enable_mpi=&quot;yes&quot;<br />with_mpi_prefix=&quot;...... etc /intel/compilers_and_libraries_2017.5.239/linux/mpi/intel64&quot;<br />with_linalg_flavor=&quot;mkl&quot;<br />with_linalg_libs=&quot; -lmkl_scalapack_ilp64 -lmkl_cdft_core -lmkl_blacs_intelmpi_ilp64 -liomp5 -lpthread -lm -ldl&quot;<br />with_fft_flavor=&quot;fftw3&quot;<br />with_fft_libs=&quot; -lmkl_scalapack_ilp64 -lmkl_cdft_core -lmkl_blacs_intelmpi_ilp64 -liomp5 -lpthread -lm -ldl&quot;<br />FCFLAGS_EXTRA=&quot; -mkl=parallel&quot;<br />CFLAGS_EXTRA=&quot; -DMKL_ILP64 -mkl=parallel&quot;<br />CXXFLAGS_EXTRA=&quot; -DMKL_ILP64 -mkl=parallel&quot;<br />with_fc_vendor=&quot;intel&quot;<br />with_fc_version=&quot;17.0.5&quot;<br />with_cc_vendor=&quot;intel&quot;<br />with_cc_version=&quot;17.0.5&quot;<br />with_cxx_vendor=&quot;intel&quot;<br />with_cxx_version=&quot;17.0.5&quot;<br />  <br />And here is some build information as printed in the log file of a calculation job.<br /><br /> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br /> <br /> === Build Information === <br />  Version       : 8.6.3<br />  Build target  : x86_64_linux_intel17.0<br />  Build date    : 20180307<br /> <br /> === Compiler Suite === <br />  C compiler       : gnu<br />  C++ compiler     : gnu17.0<br />  Fortran compiler : intel17.0<br />  CFLAGS           :  -g -O3 -mtune=native -march=native -xHost   -DMKL_ILP64 -mkl=parallel<br />  CXXFLAGS         :  -g -O3 -mtune=native -march=native -xHost   -DMKL_ILP64 -mkl=parallel<br />  FCFLAGS          :  -g   -mkl=parallel<br />  FC_LDFLAGS       : <br /> <br /> === Optimizations === <br />  Debug level        : basic<br />  Optimization level : yes<br />  Architecture       : intel_xeon<br /> <br /> === Multicore === <br />  Parallel build : yes<br />  Parallel I/O   : auto<br />  openMP support : no<br />  GPU support    : no<br /> <br /> === Connectors / Fallbacks === <br />  Connectors on : yes<br />  Fallbacks on  : yes<br />  DFT flavor    : none<br />  FFT flavor    : fftw3<br />  LINALG flavor : mkl<br />  MATH flavor   : none<br />  TIMER flavor  : abinit<br />  TRIO flavor   : none<br /> <br /> === Experimental features === <br />  Bindings            : @enable_bindings@<br />  Exports             : no<br />  GW double-precision : no<br /><br /> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br /> Default optimizations:<br />   -O3 -mtune=native -march=native -xHost<br /><br /> Optimizations for 20_datashare:<br />   -O0<br /> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br /> CPP options activated during the build:<br />                    CC_GNU                   CXX_GNU                  FC_INTEL<br /> HAVE_FC_ALLOCATABLE_DT...             HAVE_FC_ASYNC  HAVE_FC_COMMAND_ARGUMENT<br />      HAVE_FC_COMMAND_LINE        HAVE_FC_CONTIGUOUS           HAVE_FC_CPUTIME<br />             HAVE_FC_ETIME              HAVE_FC_EXIT             HAVE_FC_FLUSH<br />             HAVE_FC_GAMMA            HAVE_FC_GETENV            HAVE_FC_GETPID<br />   HAVE_FC_IEEE_EXCEPTIONS             HAVE_FC_IOMSG     HAVE_FC_ISO_C_BINDING<br />  HAVE_FC_ISO_FORTRAN_2008        HAVE_FC_LONG_LINES        HAVE_FC_MOVE_ALLOC<br />           HAVE_FC_PRIVATE         HAVE_FC_PROTECTED         HAVE_FC_STREAM_IO<br />            HAVE_FC_SYSTEM                  HAVE_FFT            HAVE_FFT_FFTW3<br />              HAVE_FFT_MPI           HAVE_FFT_SERIAL        HAVE_LIBPAW_ABINIT<br />      HAVE_LIBTETRA_ABINIT               HAVE_LINALG         HAVE_LINALG_AXPBY<br />        HAVE_LINALG_GEMM3M  HAVE_LINALG_MKL_IMATCOPY   HAVE_LINALG_MKL_OMATADD<br />  HAVE_LINALG_MKL_OMATCOPY   HAVE_LINALG_MKL_THREADS           HAVE_LINALG_MPI<br />        HAVE_LINALG_SERIAL                  HAVE_MPI                 HAVE_MPI2<br />       HAVE_MPI_IALLREDUCE        HAVE_MPI_IALLTOALL       HAVE_MPI_IALLTOALLV<br />        HAVE_MPI_INTEGER16               HAVE_MPI_IO HAVE_MPI_TYPE_CREATE_S...<br />             HAVE_OS_LINUX         HAVE_TIMER_ABINIT              USE_MACROAVE<br /> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br /><br />One thing I don't get is why the C and C++ compilers are recognized as GNU variety even though I think that I did everything to ensure they are seen as Intel's (which should perhaps be done automatically, but never mind). Probably this is not the cause of the defective inter-node paralelism, though I am not sure. <br /><br />Many thanks again if you, Eric, or someone else could share some ideas about what I may be doing wrong or could try to solve the inter-node parallelism issue.<br /><br />Dan</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: defective inter-node parallelism with 2017 Intel compile</h3>
				<div class="date">Posted: <strong>Wed Mar 21, 2018 8:09 pm</strong></div>
				<div class="author">by <strong>ebousquet</strong></div>
				<div class="content">Dear Dan,<br />We'll try to solve this problem. Intel17 has more optimizations than 2013 by default, which speedup the calculations but also can add noise that make the code diverging, mostly the relaxations is indeed affected (also observed on other codes)... <br />It is indeed strange that it does not recognize your intel C compiler, maybe you could force him by specifying the compiler directly, for example:<br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>CC=mpiicc <br />CXX=mpiicpc <br />FC=mpiifort</code></pre></div><br /><br />Could you try to compile with the following compilation flags, it helped in my case with intel17 and Xeon:<br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>FCFLAGS=&quot;-O2 -axCORE-AVX2 -xavx -mkl -fp-model precise&quot; <br />FFLAGS=&quot;-O2 -axCORE-AVX2 -xavx -mkl -fp-model precise&quot; <br />CFLAGS=&quot;-O2 -axCORE-AVX2 -xavx -mkl -fp-model precise&quot; <br />CXXFLAGS=&quot;-O2 -axCORE-AVX2 -xavx -mkl -fp-model precise&quot; </code></pre></div><br /><br />And in my case I'm using the following (sounds similar to yours):<br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>&nbsp;<br />--with-fc-vendor=intel <br />--with-fft-flavor=fftw3-mkl <br />--with-fft-libs=&quot;-lmkl_intel_lp64 -lmkl_sequential -lmkl_core&quot; <br />--with-linalg-flavor=&quot;mkl+scalapack&quot; <br />--with-linalg-libs=&quot;-lmkl_scalapack_lp64 -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -lmkl_blacs_intelmpi_lp64 -lpthread -lm -ldl&quot; <br />--enable-mpi --enable-mpi-inplace --enable-mpi-io <br />--enable-zdot-bugfix --enable-avx-safe-mode --enable-fallbacks </code></pre></div><br /><br />Let me know how it goes, mostly with the compilation flags.<br />Best wishes,<br />Eric</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: defective inter-node parallelism with 2017 Intel compile</h3>
				<div class="date">Posted: <strong>Thu Mar 22, 2018 6:09 pm</strong></div>
				<div class="author">by <strong>danric</strong></div>
				<div class="content">Hello again, Eric, and thank you.<br /><br />First, the only thing that works so that I can access mpiifort and the others (instead of mpif90, mpicc and mpicxx) is to directly change those inside the &quot;configure&quot; file. As mentioned, it does not apparently recognize mpiicc and mpiicpc as Intel compilers, but I think that these are the ones that are used for compiling. And the fortran compiler is seen as Intel's (though if I don't use the &quot;with-vendor=intel&quot; flag I still run into trouble with not copying the mods file in their directory and the build stops).<br /><br />I have tried, today, and years ago, to do the same using FC=mpiifort etc, but for some reason doing so still wraps to the GNU variety of compilers . Technically, the FC is listed as wrap-mpifc in config.log, and inside that I see the FC= as set by me, followed by &quot;export FC&quot; which then goes to the mpif90. So I concluded that it can't be helped using FC=, CC= and CXX=, and I was happy to find the solution to modify those inside &quot;configure&quot;.<br /><br />Now, back to the parallelization issue. I have tried the flags you suggested and although there are other combinations of them to try if inspiration dries out, unfortunately the one silver bullet that would solve this issue is still elusive. Although not using the exact linear algebra and fft library options that you suggested, I compiled with the flags that apparently worked in your case. Though there are some complaints/warnings when compiling many F90 programs during &quot;make&quot;, the build doesn't stop and I can get the executables. <br /><br />The single node calculation runs OK though a bit slower than before, and the 2-node still doesn't converge (perhaps I should mention that when going from 1 node to 2 nodes the variable that is doubled is the one related to the band parallelization &quot;npband&quot;).<br /><br />Here is proof that the executables obtained have indeed used the flags that worked for you.<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br />&nbsp;<br />&nbsp;=== Build Information === <br />&nbsp; Version&nbsp; &nbsp; &nbsp; &nbsp;: 8.6.3<br />&nbsp; Build target&nbsp; : x86_64_linux_intel17.0<br />&nbsp; Build date&nbsp; &nbsp; : 20180322<br />&nbsp;<br />&nbsp;=== Compiler Suite === <br />&nbsp; C compiler&nbsp; &nbsp; &nbsp; &nbsp;: gnu<br />&nbsp; C++ compiler&nbsp; &nbsp; &nbsp;: gnu17.0<br />&nbsp; Fortran compiler : intel17.0<br />&nbsp; CFLAGS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;: -O2 -axCORE-AVX2 -xavx -mkl -fp-model precise<br />&nbsp; CXXFLAGS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;: -O2 -axCORE-AVX2 -xavx -mkl -fp-model precise<br />&nbsp; FCFLAGS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : -O2 -axCORE-AVX2 -xavx -mkl -fp-model precise<br />&nbsp; FC_LDFLAGS&nbsp; &nbsp; &nbsp; &nbsp;: <br />&nbsp;<br />&nbsp;=== Optimizations === <br />&nbsp; Debug level&nbsp; &nbsp; &nbsp; &nbsp; : basic<br />&nbsp; Optimization level : yes<br />&nbsp; Architecture&nbsp; &nbsp; &nbsp; &nbsp;: intel_xeon<br />&nbsp;<br />&nbsp;=== Multicore === <br />&nbsp; Parallel build : yes<br />&nbsp; Parallel I/O&nbsp; &nbsp;: auto<br />&nbsp; openMP support : no<br />&nbsp; GPU support&nbsp; &nbsp; : no<br />&nbsp;<br />&nbsp;=== Connectors / Fallbacks === <br />&nbsp; Connectors on : yes<br />&nbsp; Fallbacks on&nbsp; : yes<br />&nbsp; DFT flavor&nbsp; &nbsp; : none<br />&nbsp; FFT flavor&nbsp; &nbsp; : fftw3<br />&nbsp; LINALG flavor : mkl<br />&nbsp; MATH flavor&nbsp; &nbsp;: none<br />&nbsp; TIMER flavor&nbsp; : abinit<br />&nbsp; TRIO flavor&nbsp; &nbsp;: none<br />&nbsp;<br />&nbsp;=== Experimental features === <br />&nbsp; Bindings&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : @enable_bindings@<br />&nbsp; Exports&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;: no<br />&nbsp; GW double-precision : no<br />&nbsp;<br />&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br />&nbsp;<br /><br />&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br />&nbsp;Default optimizations:<br />&nbsp; &nbsp;--- None ---<br /><br /><br />&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br /><br /><br />&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br />&nbsp;CPP options activated during the build:<br /><br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; CC_GNU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CXX_GNU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; FC_INTEL<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; HAVE_AVX_SAFE_MODE HAVE_FC_ALLOCATABLE_DT...&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_ASYNC<br />&nbsp;<br />&nbsp; HAVE_FC_COMMAND_ARGUMENT&nbsp; &nbsp; &nbsp; HAVE_FC_COMMAND_LINE&nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_CONTIGUOUS<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_CPUTIME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_ETIME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_EXIT<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_FLUSH&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_GAMMA&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_GETENV<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_GETPID&nbsp; &nbsp;HAVE_FC_IEEE_EXCEPTIONS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_IOMSG<br />&nbsp;<br />&nbsp; &nbsp; &nbsp;HAVE_FC_ISO_C_BINDING&nbsp; HAVE_FC_ISO_FORTRAN_2008&nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_LONG_LINES<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_MOVE_ALLOC&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_PRIVATE&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_PROTECTED<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_STREAM_IO&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_SYSTEM&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FFT<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FFT_FFTW3&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FFT_MPI&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FFT_SERIAL<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; HAVE_LIBPAW_ABINIT&nbsp; &nbsp; &nbsp; HAVE_LIBTETRA_ABINIT&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_LINALG<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_LINALG_AXPBY&nbsp; &nbsp; &nbsp; &nbsp; HAVE_LINALG_GEMM3M&nbsp; HAVE_LINALG_MKL_IMATCOPY<br />&nbsp;<br />&nbsp; &nbsp;HAVE_LINALG_MKL_OMATADD&nbsp; HAVE_LINALG_MKL_OMATCOPY&nbsp; &nbsp;HAVE_LINALG_MKL_THREADS<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_LINALG_MPI&nbsp; &nbsp; &nbsp; &nbsp; HAVE_LINALG_SERIAL&nbsp; &nbsp; &nbsp;HAVE_LINALG_ZDOTC_B*G<br />&nbsp;<br />&nbsp; &nbsp; &nbsp;HAVE_LINALG_ZDOTU_B*G&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_MPI&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_MPI2<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_MPI2_INPLACE&nbsp; &nbsp; &nbsp; &nbsp;HAVE_MPI_IALLREDUCE&nbsp; &nbsp; &nbsp; &nbsp; HAVE_MPI_IALLTOALL<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp;HAVE_MPI_IALLTOALLV&nbsp; &nbsp; &nbsp; &nbsp; HAVE_MPI_INTEGER16&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_MPI_IO<br />&nbsp;<br />&nbsp;HAVE_MPI_TYPE_CREATE_S...&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_OS_LINUX&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_TIMER_ABINIT<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; USE_MACROAVE&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br />&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br /></code></pre></div><br /><br />And I also tried a combination with my original compilation flags (there were some complaints in config.log about using -xHost and -march=native together with your suggested flags, so I removed those), with same behavior as with above options. <br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>=== Build Information === <br />&nbsp; Version&nbsp; &nbsp; &nbsp; &nbsp;: 8.6.3<br />&nbsp; Build target&nbsp; : x86_64_linux_intel17.0<br />&nbsp; Build date&nbsp; &nbsp; : 20180322<br />&nbsp;<br />&nbsp;=== Compiler Suite === <br />&nbsp; C compiler&nbsp; &nbsp; &nbsp; &nbsp;: gnu<br />&nbsp; C++ compiler&nbsp; &nbsp; &nbsp;: gnu17.0<br />&nbsp; Fortran compiler : intel17.0<br />&nbsp; CFLAGS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;:&nbsp; -g -O2 -mtune=native -axCORE-AVX2 -xavx -mkl -fp-model precise&nbsp; &nbsp;-DMKL_ILP64 -mkl=parallel<br />&nbsp; CXXFLAGS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;:&nbsp; -g -O2 -mtune=native -axCORE-AVX2 -xavx -mkl -fp-model precise&nbsp; &nbsp;-DMKL_ILP64 -mkl=parallel<br />&nbsp; FCFLAGS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :&nbsp; -g&nbsp; &nbsp;-mkl=parallel<br />&nbsp; FC_LDFLAGS&nbsp; &nbsp; &nbsp; &nbsp;: <br />&nbsp;<br />&nbsp;=== Optimizations === <br />&nbsp; Debug level&nbsp; &nbsp; &nbsp; &nbsp; : basic<br />&nbsp; Optimization level : yes<br />&nbsp; Architecture&nbsp; &nbsp; &nbsp; &nbsp;: intel_xeon<br />&nbsp;<br />&nbsp;=== Multicore === <br />&nbsp; Parallel build : yes<br />&nbsp; Parallel I/O&nbsp; &nbsp;: auto<br />&nbsp; openMP support : no<br />&nbsp; GPU support&nbsp; &nbsp; : no<br />&nbsp;<br />&nbsp;=== Connectors / Fallbacks === <br />&nbsp; Connectors on : yes<br />&nbsp; Fallbacks on&nbsp; : yes<br />&nbsp; DFT flavor&nbsp; &nbsp; : none<br />&nbsp; FFT flavor&nbsp; &nbsp; : fftw3<br />&nbsp; LINALG flavor : mkl<br />&nbsp; MATH flavor&nbsp; &nbsp;: none<br />&nbsp; TIMER flavor&nbsp; : abinit<br />&nbsp; TRIO flavor&nbsp; &nbsp;: none<br />&nbsp;<br />&nbsp;=== Experimental features === <br />&nbsp; Bindings&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : @enable_bindings@<br />&nbsp; Exports&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;: no<br />&nbsp; GW double-precision : no<br />&nbsp;<br />&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br />&nbsp;<br /><br />&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br />&nbsp;Default optimizations:<br />&nbsp; &nbsp;-O2 -mtune=native -axCORE-AVX2 -xavx -mkl -fp-model precise<br /><br /><br />&nbsp;Optimizations for 20_datashare:<br />&nbsp; &nbsp;-O0<br /><br /><br />&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br /><br /><br />&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br />&nbsp;CPP options activated during the build:<br /><br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; CC_GNU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CXX_GNU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; FC_INTEL<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; HAVE_AVX_SAFE_MODE HAVE_FC_ALLOCATABLE_DT...&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_ASYNC<br />&nbsp;<br />&nbsp; HAVE_FC_COMMAND_ARGUMENT&nbsp; &nbsp; &nbsp; HAVE_FC_COMMAND_LINE&nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_CONTIGUOUS<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_CPUTIME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_ETIME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_EXIT<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_FLUSH&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_GAMMA&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_GETENV<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_GETPID&nbsp; &nbsp;HAVE_FC_IEEE_EXCEPTIONS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_IOMSG<br />&nbsp;<br />&nbsp; &nbsp; &nbsp;HAVE_FC_ISO_C_BINDING&nbsp; HAVE_FC_ISO_FORTRAN_2008&nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_LONG_LINES<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_MOVE_ALLOC&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_PRIVATE&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_PROTECTED<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FC_STREAM_IO&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FC_SYSTEM&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FFT<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FFT_FFTW3&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_FFT_MPI&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_FFT_SERIAL<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; HAVE_LIBPAW_ABINIT&nbsp; &nbsp; &nbsp; HAVE_LIBTETRA_ABINIT&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_LINALG<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_LINALG_AXPBY&nbsp; &nbsp; &nbsp; &nbsp; HAVE_LINALG_GEMM3M&nbsp; HAVE_LINALG_MKL_IMATCOPY<br />&nbsp;<br />&nbsp; &nbsp;HAVE_LINALG_MKL_OMATADD&nbsp; HAVE_LINALG_MKL_OMATCOPY&nbsp; &nbsp;HAVE_LINALG_MKL_THREADS<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_LINALG_MPI&nbsp; &nbsp; &nbsp; &nbsp; HAVE_LINALG_SERIAL&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; HAVE_MPI<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_MPI2&nbsp; &nbsp; &nbsp; &nbsp;HAVE_MPI_IALLREDUCE&nbsp; &nbsp; &nbsp; &nbsp; HAVE_MPI_IALLTOALL<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp;HAVE_MPI_IALLTOALLV&nbsp; &nbsp; &nbsp; &nbsp; HAVE_MPI_INTEGER16&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_MPI_IO<br />&nbsp;<br />&nbsp;HAVE_MPI_TYPE_CREATE_S...&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_OS_LINUX&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;HAVE_TIMER_ABINIT<br />&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; USE_MACROAVE&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br />&nbsp;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br /><br /></code></pre></div><br /><br />I value your continued advice, with the hope that perhaps we could still identify some compilation flags that could, as a first step at least, essentially &quot;downgrade&quot; the optimized vectorization or whatever of 2017 version compilers to an earlier version such as 2013. If you can't think of other obvious things to try, learning more about the compilation flags you suggested might be a good starting point for me to find others that could help.<br /><br />On the other hand, I worry if my issues are not due to the architecture or OS etc implementation on the system I use, which could reduce the chance of finding a solution in a reasonable time. You mention that there are similar issues with other codes too, but if you and apparently most of everyone else have found solutions (otherwise there would probably be more outcry about the performance of recent Intel compilers), I guess that either the solution should soon be found in my case too, or not at all...<br /><br />Many thanks for spending your precious time to help me and I will try anything you suggest could work.<br /><br />best wishes,<br />Dan</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: defective inter-node parallelism with 2017 Intel compile</h3>
				<div class="date">Posted: <strong>Mon Mar 26, 2018 11:04 am</strong></div>
				<div class="author">by <strong>Jordan</strong></div>
				<div class="content">Hi,<br /><br />My opinion is that there is something with you MPI installation.<br />0) Remove optimization and scalapack<br />1) Check that the MPI you use was compiled with intel 17 and not an other version of intel nor gnu.<br />2) Check the optimizations of the MPI with your admin. There might be some variables used to accelerate communication and thus decrease accuracy.<br />3) Don't use you fancy flags, try the basic one (which make the configure fail) and send us the config.log file<br /><br />cheers</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: defective inter-node parallelism with 2017 Intel compile</h3>
				<div class="date">Posted: <strong>Mon Mar 26, 2018 1:35 pm</strong></div>
				<div class="author">by <strong>danric</strong></div>
				<div class="content">Hi and thank you for taking this on systematically.<br /><br />Well, I think that doing some of what you are suggesting kinda sets us back to issues I thought I found solutions for and might take even more time to figure out, but let's try.<br /><br />Alright, I removed all optimization flags (and everything that Eric suggested), and scalapack (not sure if you wanted me to give up Intel MKL and FFTW3 altogether, so for now I kept some of the options indicated by the MKL advisory), so now the .ac looks as follows:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>FC=mpiifort<br />CC=mpiicc<br />CXX=mpiicpc<br />enable_mpi=&quot;yes&quot;<br />with_mpi_prefix=&quot;/octfs/apl/intel/compilers_and_libraries_2017.5.239/linux/mpi/intel64&quot;<br />with_linalg_flavor=&quot;mkl&quot;<br />with_linalg_libs=&quot; -lmkl_cdft_core -lmkl_blacs_intelmpi_ilp64 -liomp5 -lpthread -lm -ldl&quot;<br />with_fft_flavor=&quot;fftw3&quot;<br />with_fft_libs=&quot; -lmkl_cdft_core -lmkl_blacs_intelmpi_ilp64 -liomp5 -lpthread -lm -ldl&quot;<br />FCFLAGS_EXTRA=&quot; -mkl=parallel&quot;<br />CFLAGS_EXTRA=&quot; -DMKL_ILP64 -mkl=parallel&quot;<br />CXXFLAGS_EXTRA=&quot; -DMKL_ILP64 -mkl=parallel&quot;<br /></code></pre></div><br /><br />With this, the ./configure fails at the point where it realizes that the GNU compilers (yes, Intel's are not recognized) don't have an iso_C_binding_module. The config.log in this case is here:<br /><div class="inline-attachment">
			
		
		
				<dl class="file">
			<dt><img src="images/upload_icons/txt.gif" alt="" /> <a class="postlink" href="download/file0b3e.php?id=1539">configGNU.log</a></dt>
						<dd>(168.33 KiB) Downloaded 407 times</dd>
		</dl>
		
		
			</div><br /><br />I mentioned before that I can make the ./configure recognize the mpi Intel compilers if I manually modify the &quot;configure&quot; file (instead of FC=mpiifort, CC=mpiicc, CXX=mpiicpc, I just do mpif90-&gt;mpiifort etc). Then, with the .ac below:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>enable_mpi=&quot;yes&quot;<br />with_mpi_prefix=&quot;/octfs/apl/intel/compilers_and_libraries_2017.5.239/linux/mpi/intel64&quot;<br />with_linalg_flavor=&quot;mkl&quot;<br />with_linalg_libs=&quot; -lmkl_cdft_core -lmkl_blacs_intelmpi_ilp64 -liomp5 -lpthread -lm -ldl&quot;<br />with_fft_flavor=&quot;fftw3&quot;<br />with_fft_libs=&quot; -lmkl_cdft_core -lmkl_blacs_intelmpi_ilp64 -liomp5 -lpthread -lm -ldl&quot;<br />FCFLAGS_EXTRA=&quot; -mkl=parallel&quot;<br />CFLAGS_EXTRA=&quot; -DMKL_ILP64 -mkl=parallel&quot;<br />CXXFLAGS_EXTRA=&quot; -DMKL_ILP64 -mkl=parallel&quot;<br /></code></pre></div><br /><br />I get the following config.log file:<br /><div class="inline-attachment">
			
		
		
				<dl class="file">
			<dt><img src="images/upload_icons/txt.gif" alt="" /> <a class="postlink" href="download/filec252.php?id=1540">configINTEL.log</a></dt>
						<dd>(182.12 KiB) Downloaded 420 times</dd>
		</dl>
		
		
			</div><br /><br />In this case, the ./configure completes. According to the config.log file, the mpiicc, mpiifort and mpiicpc are apparently used as intended, yet the compiler type is mistaken: GNU type for C and C++ and Generic0.0 for Fortran. <br /><br />It is particularly critical, I think,  that the fortran compiler type is not recognized at Intel's, because the &quot;make&quot; stops soon after starting, at the point below:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>&#91;...&#93;<br />ranlib lib14_hidewrite.a<br />make&#91;1&#93;: ?????? `/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/src/14_hidewrite' ?????<br />cd src/16_hideleave &amp;&amp; make lib16_hideleave.a<br />make&#91;1&#93;: ?????? `/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/src/16_hideleave' ?????<br />/octfs/apl/intel/compilers_and_libraries_2017.5.239/linux/mpi/intel64/bin/mpiifort -DHAVE_CONFIG_H -I. -I../..&nbsp; -I../../src/10_dumpinfo -I../../src/10_dumpinfo -I../../src/12_hide_mpi -I../../src/12_hide_mpi -I../../src/11_memory_mpi -I../../src/11_memory_mpi -I../../src/10_defs -I../../src/10_defs -I../../src/14_hidewrite -I../../src/14_hidewrite -I../../src/incs -I../../src/incs -I/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/fallbacks/exports/include&nbsp; &nbsp; -g&nbsp; &nbsp;-mkl=parallel&nbsp; -c -o leave_new.o leave_new.F90<br />/octfs/apl/intel/compilers_and_libraries_2017.5.239/linux/mpi/intel64/bin/mpiifort -DHAVE_CONFIG_H -I. -I../..&nbsp; -I../../src/10_dumpinfo -I../../src/10_dumpinfo -I../../src/12_hide_mpi -I../../src/12_hide_mpi -I../../src/11_memory_mpi -I../../src/11_memory_mpi -I../../src/10_defs -I../../src/10_defs -I../../src/14_hidewrite -I../../src/14_hidewrite -I../../src/incs -I../../src/incs -I/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/fallbacks/exports/include&nbsp; &nbsp; -g&nbsp; &nbsp;-mkl=parallel&nbsp; -c -o m_xieee.o m_xieee.F90<br />/octfs/apl/intel/compilers_and_libraries_2017.5.239/linux/mpi/intel64/bin/mpiifort -DHAVE_CONFIG_H -I. -I../..&nbsp; -I../../src/10_dumpinfo -I../../src/10_dumpinfo -I../../src/12_hide_mpi -I../../src/12_hide_mpi -I../../src/11_memory_mpi -I../../src/11_memory_mpi -I../../src/10_defs -I../../src/10_defs -I../../src/14_hidewrite -I../../src/14_hidewrite -I../../src/incs -I../../src/incs -I/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/fallbacks/exports/include&nbsp; &nbsp; -g&nbsp; &nbsp;-mkl=parallel&nbsp; -c -o interfaces_16_hideleave.o interfaces_16_hideleave.F90<br />/octfs/apl/intel/compilers_and_libraries_2017.5.239/linux/mpi/intel64/bin/mpiifort -DHAVE_CONFIG_H -I. -I../..&nbsp; -I../../src/10_dumpinfo -I../../src/10_dumpinfo -I../../src/12_hide_mpi -I../../src/12_hide_mpi -I../../src/11_memory_mpi -I../../src/11_memory_mpi -I../../src/10_defs -I../../src/10_defs -I../../src/14_hidewrite -I../../src/14_hidewrite -I../../src/incs -I../../src/incs -I/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/fallbacks/exports/include&nbsp; &nbsp; -g&nbsp; &nbsp;-mkl=parallel&nbsp; -c -o m_errors.o m_errors.F90<br />rm -f lib16_hideleave.a<br />ar rc lib16_hideleave.a leave_new.o m_xieee.o m_errors.o interfaces_16_hideleave.o<br />ranlib lib16_hideleave.a<br />make&#91;1&#93;: ?????? `/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/src/16_hideleave' ?????<br />cd src/17_libtetra_ext &amp;&amp; make lib17_libtetra_ext.a<br />make&#91;1&#93;: ?????? `/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/src/17_libtetra_ext' ?????<br />/octfs/apl/intel/compilers_and_libraries_2017.5.239/linux/mpi/intel64/bin/mpiifort -DHAVE_CONFIG_H -I. -I../..&nbsp; -I../../src/11_memory_mpi -I../../src/11_memory_mpi -I../../src/16_hideleave -I../../src/16_hideleave -I../../src/14_hidewrite -I../../src/14_hidewrite -I../../src/incs -I../../src/incs -I/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/fallbacks/exports/include&nbsp; &nbsp; -g&nbsp; &nbsp;-mkl=parallel&nbsp; -c -o m_kptrank.o m_kptrank.F90<br />/octfs/apl/intel/compilers_and_libraries_2017.5.239/linux/mpi/intel64/bin/mpiifort -DHAVE_CONFIG_H -I. -I../..&nbsp; -I../../src/11_memory_mpi -I../../src/11_memory_mpi -I../../src/16_hideleave -I../../src/16_hideleave -I../../src/14_hidewrite -I../../src/14_hidewrite -I../../src/incs -I../../src/incs -I/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/fallbacks/exports/include&nbsp; &nbsp; -g&nbsp; &nbsp;-mkl=parallel&nbsp; -c -o interfaces_17_libtetra_ext.o interfaces_17_libtetra_ext.F90<br />m_kptrank.F90(29): ??? #7002: ????????????????????????????INCLUDE ????????????&nbsp; &nbsp;&#91;DEFS_BASIS&#93;<br />&nbsp;use m_profiling_abi<br />-----^<br />m_kptrank.F90(30): ??? #6580: ??????????????????&nbsp; &nbsp;&#91;MSG_HNDL&#93;<br />&nbsp;use m_errors, only : msg_hndl<br />----------------------^<br />m_kptrank.F90(160): ??? #6632: ?????????????????????????????&nbsp; &nbsp;&#91;FILE&#93;<br />&nbsp; &nbsp; &nbsp;call msg_hndl(msg,&quot;ERROR&quot;, &quot;PERS&quot; ,file=&quot;m_kptrank.F90&quot;, line=160)<br />----------------------------------------^<br />m_kptrank.F90(160): ??? #6632: ?????????????????????????????&nbsp; &nbsp;&#91;LINE&#93;<br />&nbsp; &nbsp; &nbsp;call msg_hndl(msg,&quot;ERROR&quot;, &quot;PERS&quot; ,file=&quot;m_kptrank.F90&quot;, line=160)<br />--------------------------------------------------------------^<br />m_kptrank.F90(171): ??? #6632: ?????????????????????????????&nbsp; &nbsp;&#91;FILE&#93;<br />&nbsp; &nbsp; &nbsp;call msg_hndl(msg,&quot;ERROR&quot;, &quot;PERS&quot; ,file=&quot;m_kptrank.F90&quot;, line=171)<br />----------------------------------------^<br />m_kptrank.F90(171): ??? #6632: ?????????????????????????????&nbsp; &nbsp;&#91;LINE&#93;<br />&nbsp; &nbsp; &nbsp;call msg_hndl(msg,&quot;ERROR&quot;, &quot;PERS&quot; ,file=&quot;m_kptrank.F90&quot;, line=171)<br />--------------------------------------------------------------^<br />m_kptrank.F90(288): ??? #6406: ?????????????????????????&nbsp; &nbsp;&#91;MSG_HNDL&#93;<br />&nbsp; &nbsp;call msg_hndl(msg,&quot;ERROR&quot;, &quot;PERS&quot; ,file=&quot;m_kptrank.F90&quot;, line=288)<br />--------^<br />?????? m_kptrank.F90 ????????? (??? 1)?<br />make&#91;1&#93;: *** &#91;m_kptrank.o&#93; ??? 1<br />make&#91;1&#93;: ?????? `/octfs/home/d45678/abinit/v863_octG/abinit-8.6.3/src/17_libtetra_ext' ?????<br />make: *** &#91;abinit&#93; ??? 2<br /></code></pre></div><br /><br />Several years ago I figured out that I can still obtain the executables if I manually copy all the .mod files that are created in each directory of /src to /incs or /mods directory. So doing allows me to obtain the executables, yet the compile type of &quot;abinit&quot; will be marked as &quot;prepared for a x86_64_linux_Generic0.0 computer&quot;<br /><br />I have learned to avoid the hassle of copying all those files manually (and have the &quot;abinit&quot; recognized as compiler under Intel compilers) simply by adding the below entries to the .ac file (it might be enough to do this for Fortran, but haven't tried; naturally the version number changes according to the one actually used):<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>with_fc_vendor=&quot;intel&quot;<br />with_fc_version=&quot;17.0.5&quot;<br />with_cc_vendor=&quot;intel&quot;<br />with_cc_version=&quot;17.0.5&quot;<br />with_cxx_vendor=&quot;intel&quot;<br />with_cxx_version=&quot;17.0.5&quot;<br /></code></pre></div><br /><br />Doing the latter, allows me to obtain the executables with behavior as described in the previous posts. (Well, I have yet to try without any optimizations for the latest version of abinit, and will report the result soon, but I somehow doubt that simply removing the optimizations and scalapack is likely to be the solution that I was looking for... could be wrong though, will try and report)<br /><br />Please advise further when you have a moment to consider these. <br />Thank you and best,<br />Dan<br /><br />PS I have also asked my admin's help, but didn't get very far yet, which is why I decided to ask here as well. Will confirm the other MPI-related questions with them soon too.</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: defective inter-node parallelism with 2017 Intel compile</h3>
				<div class="date">Posted: <strong>Mon Mar 26, 2018 5:21 pm</strong></div>
				<div class="author">by <strong>jbeuken</strong></div>
				<div class="content">Hi,<br /><br />I successfully run on a cluster with 1, 2, 4 and 8 nodes ( 16 cores each ) with version 8.4.2, 8.5.0 and 8.7.3<br />the version of intel compiler is clusterstudio 2017.3.191<br /><br />this is my ac file :<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code># clusterstudio&nbsp; &nbsp;ifort 17<br /><br /># Fortran compiler<br /># ================<br />FC=&quot;mpiifort&quot;<br />CC=&quot;mpiicc&quot;<br />CXX=&quot;mpiicpc&quot;<br />AR=ar<br /><br /># Fortran optimization flags<br /># ==========================<br />FCFLAGS_EXTRA=&quot;-g -O3 -align all&quot;<br />enable_optim=&quot;yes&quot;<br />enable_gw_dpc=&quot;yes&quot;<br />enable_64bit_flags=&quot;yes&quot;<br /><br />enable_openmp=&quot;yes&quot;<br />FCFLAGS_OPENMP=&quot;-openmp&quot;<br /><br /># Parallel compilation flags<br /># ==========================<br />enable_mpi=&quot;yes&quot;<br />enable_mpi_io=&quot;yes&quot;<br /># I_MPI_ROOT=/opt/software/intel/impi/2017.2.191<br />with_mpi_incs=&quot;-I${I_MPI_ROOT}/include64&quot;<br />with_mpi_libs=&quot;-L${I_MPI_ROOT}/lib64 -lmpi&quot;<br /><br /># Linear Algebra library links (ScaLAPACK)<br /># ========================================<br />with_linalg_flavor=&quot;mkl+scalapack&quot;<br />with_linalg_incs=&quot;-I${MKLROOT}/include&quot;<br />with_linalg_libs=&quot;-L${MKLROOT}/lib/intel64 -lmkl_scalapack_lp64 -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -lmkl_blacs_intelmpi_lp64 -liomp5 -lpthread -lm -ldl&quot;<br /><br /># FFTW3 / MKL <br /># ========================================<br />with_fft_flavor=&quot;dfti&quot;<br />with_fft_libs=&quot;-L${MKLROOT}/lib/intel64 -lmkl_scalapack_lp64 -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -lmkl_blacs_intelmpi_lp64 -liomp5 -lpthread -lm -ldl&quot;<br />with_fft_incs=&quot;-I${MKLROOT}/include&quot;<br /><br /># Plugins additionels<br /># ===================<br />with_dft_flavor=&quot;libxc&quot;<br />with_libxc_incs=&quot;-I${HOME}/local/libxc/include&quot;<br />with_libxc_libs=&quot;-L${HOME}/local/libxc/lib -lxcf90 -lxc&quot;<br /><br />with_trio_flavor=&quot;netcdf&quot;<br />with_netcdf_incs=&quot;-I${HOME}/local/netcdf/include -I${HOME}/local/hdf5/include&quot;<br />with_netcdf_libs=&quot;-L${HOME}/local/netcdf/lib -lnetcdff -lnetcdf -L${HOME}/local/hdf5/lib -lhdf5_hl -lhdf5&quot;<br />enable_netcdf_default=&quot;yes&quot;<br /><br /></code></pre></div><br /><br /><br />and , this is a part of my input file ( UO2 54 Atoms )<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>#ABINIT - INPUT FILE<br />#UO2 54 ATOMS<br /><br />#Process distribution (parallelism) - TO BE ADAPTED<br /># autoparal 1<br /># npkpt 2 npband 4 npfft 4 # 32 processeurs<br /># npkpt 2 npband 8 npfft 4 # 64 processeurs -1<br /># npkpt 2 npband 4 npfft 8 # 64 processeurs -2<br />&nbsp;npkpt 2 npband 8 npfft 8 # 128 processeurs<br />paral_kgb 1<br />bandpp 2<br /><br />#fftalg 512<br /><br />#Plane wave basis<br />ecut 6.<br />pawecutdg 10.<br /><br />#Self-consistent cycle parameters<br />toldfe 1.d-5<br />nstep 30<br />nline 6<br />diemac 10.<br /><br />#K-points and symetries<br />nkpt 1<br />kpt 0.5 0.5 0.5<br />kptopt 0<br />nsym 0<br />maxnsym 2048<br />chksymbreak 0<br /><br />#Electronic configuration<br />nsppol 2<br />nband 256<br />occopt 3<br />tsmear 0.0005 hartree<br /><br />...</code></pre></div><br /><br />my 5Â¢<br /><br />regards<br /><br />jmb</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: defective inter-node parallelism with 2017 Intel compile</h3>
				<div class="date">Posted: <strong>Mon Mar 26, 2018 6:05 pm</strong></div>
				<div class="author">by <strong>danric</strong></div>
				<div class="content">Dear JMB<br /><br />Thank you. There are a few things worth trying there, so I will and report if I can get it to work.<br /><br />Many thanks again,<br />Dan</div>
			</div>
			<hr />
			</div>

	<div id="page-footer" class="page-footer">
		<div class="page-number">All times are <span title="Europe/Brussels">UTC+02:00</span><br />Page <strong>1</strong> of <strong>1</strong></div>
			<div class="copyright">
				<p>Powered by <a href="https://www.phpbb.com/">phpBB</a>&reg; Forum Software &copy; phpBB Limited
				</p>
							</div>
	</div>
</div>

</body>

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=3&t=3801&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 20 Sep 2024 23:12:25 GMT -->
</html>
