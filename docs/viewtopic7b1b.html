<!DOCTYPE html>
<html dir="ltr" lang="en-gb">

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=9&t=1584&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 03:51:40 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="robots" content="noindex" />

<title>ABINIT Discussion Forums &bull; Abinit 6.12.2 hanging on WFK write with MPI-IO</title>

<link href="styles/flat-style/theme/print.css" rel="stylesheet">
<link href="styles/flat-style/theme/bidi.css" rel="stylesheet">
</head>
<body id="phpbb" class="ltr">
<div id="wrap" class="wrap">
	<a id="top" class="top-anchor" accesskey="t"></a>

	<div id="page-header">
		<h1>ABINIT Discussion Forums</h1>
		<p>The meeting place for ABINIT users and developers<br /><a href="index.html">https://forum.abinit.org/</a></p>

		<h2>Abinit 6.12.2 hanging on WFK write with MPI-IO</h2>
		<p><a href="viewtopice8db.html?f=9&amp;t=1584">https://forum.abinit.org/viewtopic.php?f=9&amp;t=1584</a></p>
	</div>

	<div id="page-body" class="page-body">
		<div class="page-number">Page <strong>1</strong> of <strong>1</strong></div>
					<div class="post">
				<h3>Abinit 6.12.2 hanging on WFK write with MPI-IO</h3>
				<div class="date">Posted: <strong>Tue Mar 27, 2012 4:43 am</strong></div>
				<div class="author">by <strong>kaneod</strong></div>
				<div class="content">I'm in a bit of a bind. I need the WFK files for a series of rather large molecules (38-173 atoms) so I can visualize the HOMO/LUMO for each. Since I'm only running a gamma point calculation, I'm using KGB parallelization over up to 640 bands and over up to 48 processors, 8 processors per node. Temporary files are being written to each node's tmp area but the collective files (outputs) are written to a fast GPFS filesystem.<br /><br />If I compile MPI-IO support into Abinit, the jobs hang at the stage of writing the WFK files. The last log output is (for any sized molecule):<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>&nbsp;----iterations are completed or convergence reached----<br /><br />&nbsp;outwf&nbsp; : write wavefunction to file poro_WFK<br />-P-0000&nbsp; leave_test : synchronization done...<br /><br />&nbsp;m_wffile.F90:279:COMMENT<br />&nbsp; &nbsp;MPI/IO accessing FORTRAN file header: detected record mark length=4</code></pre></div><br /><br />The code does not proceed any further although the processor utilization appears to stay at near 100% of normal (typically at least 95% over all the processors/nodes). The WFK file as written is about 800-850K in size, I suppose it just contains a header.<br /><br />Technical details: the code is compiled using gfortran 4.6.2, fftw3, openblas, against either OpenMPI 1.4.5 or mvapich2 (1.8 and HEAD). Same error regardless of MPI implementation. This is odd, as mvapich2 has the type error problem still when compiling with respect to mpi level 2, so it is necessary to use mpi level 1 to get abinit compiled. Since the MPI-IO routines aren't actually in mpi.h as far as I'm aware, you'd think something would break catastrophically for mvapich2 and MPI-IO.<br /><br />If I disable MPI-IO, the jobs complete but the WFK files are incomplete. They are larger (at least several MB) but not large enough to contain all the cgs, especially for the larger molecules.<br /><br />Note that the DEN files never have any trouble writing, but it doesn't appear to be a filesize problem (as has been hinted in previous threads) because the hang occurs regardless of the molecule (for example, the 38-atom porphine core attached as an example input fails).<br /><br />Another thing is that I <strong class="text-strong">don't have this problem on my workstation</strong> with MPI-IO enabled and using MPICH2. My molecules are too big for my workstation to deal with in a reasonable amount of time. Possibly mpi-io is a bit broken on our cluster, but what steps should I take to try and isolate this further for the developers?<br /><br />I've pasted in an input file, build ac file and jobscript for our cluster that will reproduce the problem. Still can't add attachments (even with txt extensions...).<br /><br />Thanks,<br /><br />Kane<br /><br />Input file:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code># porphin molecule<br /><br /><br />#iscf&nbsp; &nbsp; &nbsp; &nbsp; -3<br />#occopt&nbsp; &nbsp; &nbsp; 1<br />nband&nbsp; &nbsp; &nbsp; &nbsp;320<br />#nbdbuf&nbsp; &nbsp; &nbsp; 160<br /><br /># DOS stuff<br /><br />#prtdos&nbsp; &nbsp; &nbsp; 3<br />#natsph&nbsp; &nbsp; &nbsp; 20<br />#iatsph&nbsp; &nbsp; &nbsp; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20<br />#pawprtdos&nbsp; &nbsp;2<br /><br /># SCF parameters<br /><br />ecut&nbsp; &nbsp; &nbsp; &nbsp; 14.0<br />pawecutdg&nbsp; &nbsp;35.0<br />#toldff&nbsp; &nbsp; &nbsp; 5.0d-6<br />#toldfe&nbsp; &nbsp; &nbsp; 4.0d-8&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Tighten up convergence slightly<br />tolvrs&nbsp; &nbsp; &nbsp; 1.0d-10<br />nstep&nbsp; &nbsp; &nbsp; &nbsp;200<br />#istwfk&nbsp; &nbsp; &nbsp; 2&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Gamma-point only means real wfs.<br />#occopt&nbsp; &nbsp; &nbsp; 7&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Gaussian smearing<br />#tsmear&nbsp; &nbsp; &nbsp; 0.02<br />diemix&nbsp; &nbsp; &nbsp; 0.33<br />diemac&nbsp; &nbsp; &nbsp; 2.0<br />#timopt&nbsp; &nbsp; &nbsp;2<br /><br /># Geometry Optimization<br /><br />#optcell&nbsp; &nbsp; &nbsp;0<br />#ionmov&nbsp; &nbsp; &nbsp;2<br />#tolmxf&nbsp; &nbsp; &nbsp;5.0d-5<br />#ntime&nbsp; &nbsp; &nbsp; 100&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br /><br /># Kpoints<br /><br /># Note: for DOS, need more than gamma point.<br /><br />kptopt&nbsp; &nbsp; &nbsp; 1<br />ngkpt&nbsp; &nbsp; &nbsp; &nbsp;1&nbsp; &nbsp;1&nbsp; &nbsp;1<br />#ngkpt&nbsp; &nbsp; &nbsp; &nbsp;2 2 2&nbsp; &nbsp; &nbsp;# For DOS<br />nshiftk&nbsp; &nbsp; &nbsp;1<br />shiftk<br />&nbsp; &nbsp; 0.0 0.0 0.0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Need to use the true gamma point.<br />#nband&nbsp; &nbsp; &nbsp; &nbsp;80&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Done to fix an input bug.<br /><br /># Parallelization<br /><br />paral_kgb&nbsp; &nbsp; &nbsp; &nbsp;1<br />npkpt&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1<br />npspinor&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1<br />npband&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8<br />npfft&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2<br />bandpp&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4<br /><br /># Geometry<br /><br />acell&nbsp; &nbsp; &nbsp; &nbsp;24.0&nbsp; &nbsp; 24.0&nbsp; &nbsp; 14.0&nbsp; &nbsp; &nbsp;angstrom<br />natom&nbsp; &nbsp; &nbsp; &nbsp;38<br />ntypat&nbsp; &nbsp; &nbsp; 3<br />znucl&nbsp; &nbsp; &nbsp; &nbsp;6 7 1<br />typat&nbsp; &nbsp; &nbsp; &nbsp;1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1 1 1 1 2 2 2 2 3 3 3 3 3 3 3 3 <br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3 3 3 3 3 3<br />xangst<br />14.43267826697125&nbsp; &nbsp; &nbsp;9.54719469654066&nbsp; &nbsp; &nbsp;6.98679555192892<br />9.56971890234498&nbsp; &nbsp; &nbsp;9.55740143113795&nbsp; &nbsp; &nbsp;6.98136687410452<br />9.57830669996704&nbsp; &nbsp; 14.45323690496433&nbsp; &nbsp; &nbsp;6.99729471155192<br />14.44063286032851&nbsp; &nbsp; 14.44280830865907&nbsp; &nbsp; &nbsp;7.00065218638275<br />13.09053535559251&nbsp; &nbsp; &nbsp;9.13328020797658&nbsp; &nbsp; &nbsp;6.98306316607677<br />12.67923156052460&nbsp; &nbsp; &nbsp;7.73168327773270&nbsp; &nbsp; &nbsp;6.97936199186946<br />11.31569250083367&nbsp; &nbsp; &nbsp;7.73433869282024&nbsp; &nbsp; &nbsp;6.97757649970769<br />10.90999275359248&nbsp; &nbsp; &nbsp;9.13761393307977&nbsp; &nbsp; &nbsp;6.98028668551213<br />9.09554011542721&nbsp; &nbsp; 10.87314035126708&nbsp; &nbsp; &nbsp;6.98561843864410<br />7.72903461616869&nbsp; &nbsp; 11.31936915287400&nbsp; &nbsp; &nbsp;6.98735375088374<br />7.73139760192924&nbsp; &nbsp; 12.69760130406586&nbsp; &nbsp; &nbsp;6.99113573510774<br />9.09938031597771&nbsp; &nbsp; 13.13925447483067&nbsp; &nbsp; &nbsp;6.99243851467787<br />10.92010300631924&nbsp; &nbsp; 14.86758534532777&nbsp; &nbsp; &nbsp;7.00085421660784<br />11.33151776548370&nbsp; &nbsp; 16.26917592327966&nbsp; &nbsp; &nbsp;7.00566526340061<br />12.69511949460426&nbsp; &nbsp; 16.26616429625911&nbsp; &nbsp; &nbsp;7.00672787336548<br />13.10041479027480&nbsp; &nbsp; 14.86264378806152&nbsp; &nbsp; &nbsp;7.00242954254950<br />14.91526145037882&nbsp; &nbsp; 13.12729369433697&nbsp; &nbsp; &nbsp;6.99679680639886<br />16.28188688560975&nbsp; &nbsp; 12.68157456405687&nbsp; &nbsp; &nbsp;6.99612434170756<br />16.28001488882063&nbsp; &nbsp; 11.30329858626045&nbsp; &nbsp; &nbsp;6.99265482691192<br />14.91210592507385&nbsp; &nbsp; 10.86110781146765&nbsp; &nbsp; &nbsp;6.99071674837176<br />14.13078870987637&nbsp; &nbsp; 11.99535250594183&nbsp; &nbsp; &nbsp;6.99326588288911<br />9.88047357875540&nbsp; &nbsp; 12.00482853795923&nbsp; &nbsp; &nbsp;6.98882891763749<br />12.00192233673013&nbsp; &nbsp; &nbsp;9.96929220396100&nbsp; &nbsp; &nbsp;6.98329443280378<br />12.00837527590531&nbsp; &nbsp; 14.03130874233860&nbsp; &nbsp; &nbsp;6.99948340730862<br />15.20432290532285&nbsp; &nbsp; 15.22540747831387&nbsp; &nbsp; &nbsp;7.00200372856651<br />15.19313884171311&nbsp; &nbsp; &nbsp;8.76129846997055&nbsp; &nbsp; &nbsp;6.98744336798576<br />8.80561700326767&nbsp; &nbsp; &nbsp;8.77507783974137&nbsp; &nbsp; &nbsp;6.97949471039301<br />8.81771718378007&nbsp; &nbsp; 15.23885330665432&nbsp; &nbsp; &nbsp;6.99845713352080<br />13.37257765050554&nbsp; &nbsp; 17.11992092452597&nbsp; &nbsp; &nbsp;7.01033475042726<br />10.65751847151932&nbsp; &nbsp; 17.12561065649051&nbsp; &nbsp; &nbsp;7.00718932911153<br />17.14387397433472&nbsp; &nbsp; 13.34609132569375&nbsp; &nbsp; &nbsp;6.99830189384125<br />17.14042276570114&nbsp; &nbsp; 10.63682513020204&nbsp; &nbsp; &nbsp;6.99118969243677<br />13.35278112781582&nbsp; &nbsp; &nbsp;6.87498761980712&nbsp; &nbsp; &nbsp;6.97878434452385<br />10.63876867003055&nbsp; &nbsp; &nbsp;6.88026233515408&nbsp; &nbsp; &nbsp;6.97553810680641<br />6.87131218204644&nbsp; &nbsp; 13.36438702790796&nbsp; &nbsp; &nbsp;6.99316119368211<br />6.86695286671293&nbsp; &nbsp; 10.65501172582219&nbsp; &nbsp; &nbsp;6.98633580459881<br />10.90483155436783&nbsp; &nbsp; 12.00301405431957&nbsp; &nbsp; &nbsp;6.99005188546697<br />13.10645914541739&nbsp; &nbsp; 11.99653337020754&nbsp; &nbsp; &nbsp;6.99332769226049</code></pre></div><br /><br />Build ac file:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>enable_64bit_flags=&quot;yes&quot;<br />#enable_optim=&quot;yes&quot;<br />enable_debug=&quot;no&quot;<br />prefix=&quot;${HOME}/local&quot;<br /><br />CPP=&quot;$HOME/local/bin/cpp-4.6&quot;<br />CC=&quot;$HOME/local/bin/mpicc&quot;<br />#CFLAGS_OPTIM=&quot;-O3 -march=native&quot;<br />#C_LDFLAGS=&quot;-static-libgcc&quot;<br />CXX=&quot;$HOME/local/bin/mpicxx&quot;<br />#CXXFLAGS_OPTIM=&quot;-O3 -march=native&quot;<br />FC=&quot;$HOME/local/bin/mpif90&quot;<br />F77=&quot;$HOME/local/bin/mpif77&quot;<br />#FCFLAGS_DEBUG=&quot;-g -fopenmp&quot;<br />#FCFLAGS_OPTIM=&quot;-O3 -march=native -mtune=native -funroll-loops -floop-block -flto&quot;<br />#FC_LDFLAGS=&quot;-flto -static-libgfortran -static-libgcc&quot;<br />FC_LIBS_EXTRA=&quot;-lgomp&quot;<br /><br />#enable_stdin=&quot;no&quot;<br />#fcflags_opt_59_io_mpi=&quot;-O2&quot;<br />#fcflags_opt_51_manage_mpi=&quot;-O2&quot;<br /><br />enable_mpi=&quot;yes&quot;<br />enable_mpi_io=&quot;yes&quot;<br />with_mpi_level=&quot;1&quot;<br />MPI_RUNNER=&quot;$HOME/local/bin/mpirun&quot;<br /><br />with_trio_flavor=&quot;netcdf+etsf_io&quot;<br />#with_etsf_io_incs=&quot;-I/opt/etsf/include&quot;<br />#with_etsf_io_libs=&quot;-L/opt/etsf/lib -letsf_io&quot;<br />#with_netcdf_incs=&quot;-I/usr/local/include/netcdf&quot;<br />#with_netcdf_libs=&quot;-L/usr/local/lib/netcdf -lnetcdff -lnetcdf&quot;<br /><br />with_fft_flavor=&quot;fftw3&quot;<br />with_fft_incs=&quot;-I$HOME/local/include&quot;<br />with_fft_libs=&quot;-L$HOME/local/lib -lfftw3&quot;<br /><br />with_linalg_flavor=&quot;custom&quot;<br />with_linalg_incs=&quot;-I$HOME/local/include&quot;<br />with_linalg_libs=&quot;-L$HOME/local/lib -lopenblas&quot;<br /><br />with_dft_flavor=&quot;atompaw+libxc+wannier90&quot;<br /><br />with_libxc_incs=&quot;-I$HOME/local/include&quot;<br />with_libxc_libs=&quot;-L$HOME/local/lib -lxc&quot;<br /><br />#enable_fallbacks=&quot;no&quot;<br /><br />#enable_gw_cutoff=&quot;yes&quot;<br />enable_gw_dpc=&quot;yes&quot;<br />enable_gw_openmp=&quot;yes&quot;<br />#enable_gw_optimal=&quot;yes&quot;<br />#enable_gw_wrapper=&quot;yes&quot;<br />enable_smp=&quot;yes&quot;<br /><br />enable_fast_check=&quot;yes&quot;</code></pre></div><br /><br />Jobscript&#058;<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>#!/bin/bash<br /><br />#PBS -S /bin/bash<br />#PBS -N porphin-ORB1<br />#PBS -l nodes=2:ppn=8,walltime=04:00:00,pmem=4000MB<br />#PBS -joe<br />#PBS -V<br /><br />EXTRA_FILES=<br />ABINIT=$HOME/test/bin/abinit<br />MPIEXEC=&quot;$HOME/local/bin/mpiexec-osc&quot;<br />#export LD_LIBRARY_PATH=$HOME/test/lib64:$HOME/test/lib:$LD_LIBRARY_PATH<br /><br />cd $PBS_O_WORKDIR<br /><br /><br /># Temporary directory<br />export TMPDIR=&quot;$HOME/ASync002_scratch&quot;<br /># Work directory<br />export WORKDIR=&quot;$HOME/ASync002_scratch/${PBS_JOBID}-abinit-${PBS_JOBNAME}&quot;<br />mkdir $WORKDIR<br /><br /># Copy ab.files and the input file into the work directory and go there.<br />cp ab.files *.in $EXTRA_FILES $WORKDIR<br />cd $WORKDIR<br /><br /># Modify ab.files so that the tmp files are always local - PBS<br /># or mpiexec creates a directory /tmp/$PBS_JOBID on each node.<br />sed -i &quot;s_&gt;REPLACE&lt;_/tmp/${PBS_JOBID}/tmp_&quot; ab.files<br /><br /># Make sure we explicitly set OMP_NUM_THREADS or who knows<br /># what OpenMP nightmares will happen.<br />export OMP_NUM_THREADS=1<br /><br /># Get our nodefile and write it to this directory for reference.<br />cp $PBS_NODEFILE .<br /><br /># Now run (hopefully PBS gives us the right nodes!)<br />$MPIEXEC $ABINIT &lt; ab.files &gt;&amp; log<br /><br /># Move the job folder to the original submission dir.<br />cd ..<br />mv $WORKDIR $PBS_O_WORKDIR</code></pre></div></div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Abinit 6.12.2 hanging on WFK write with MPI-IO</h3>
				<div class="date">Posted: <strong>Thu Mar 29, 2012 2:52 am</strong></div>
				<div class="author">by <strong>kaneod</strong></div>
				<div class="content">Well, I'll do a self-response on this: it seems that even with paral_kgb=1, where the code complains that I shouldn't be able to do input/output, I get proper WFK files by compiling abinit without MPI-IO. So for now, I'm just running without it. Performance isn't too bad.<br /><br />...<br /><br />I'm sorry I can't shed more light on this as there do seem to be some problems writing WFK files in a variety of circumstances (do a WFK search on the forums) and they don't seem to be solved in 6.12.2 for some people. Is there anything the developers would like me to do to help check this in more detail?<br /><br />Kane</div>
			</div>
			<hr />
			</div>

	<div id="page-footer" class="page-footer">
		<div class="page-number">All times are <span title="Europe/Brussels">UTC+02:00</span><br />Page <strong>1</strong> of <strong>1</strong></div>
			<div class="copyright">
				<p>Powered by <a href="https://www.phpbb.com/">phpBB</a>&reg; Forum Software &copy; phpBB Limited
				</p>
							</div>
	</div>
</div>

</body>

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=9&t=1584&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 03:51:40 GMT -->
</html>
