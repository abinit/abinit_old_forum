<!DOCTYPE html>
<html dir="ltr" lang="en-gb">

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=9&t=1703&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 03:51:40 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="robots" content="noindex" />

<title>ABINIT Discussion Forums &bull; parallel SCF crash for big systems (also with prtwf=0)</title>

<link href="styles/flat-style/theme/print.css" rel="stylesheet">
<link href="styles/flat-style/theme/bidi.css" rel="stylesheet">
</head>
<body id="phpbb" class="ltr">
<div id="wrap" class="wrap">
	<a id="top" class="top-anchor" accesskey="t"></a>

	<div id="page-header">
		<h1>ABINIT Discussion Forums</h1>
		<p>The meeting place for ABINIT users and developers<br /><a href="index.html">https://forum.abinit.org/</a></p>

		<h2>parallel SCF crash for big systems (also with prtwf=0)</h2>
		<p><a href="viewtopic5d5b.html?f=9&amp;t=1703">https://forum.abinit.org/viewtopic.php?f=9&amp;t=1703</a></p>
	</div>

	<div id="page-body" class="page-body">
		<div class="page-number">Page <strong>1</strong> of <strong>1</strong></div>
					<div class="post">
				<h3>parallel SCF crash for big systems (also with prtwf=0)</h3>
				<div class="date">Posted: <strong>Mon Jun 18, 2012 4:28 pm</strong></div>
				<div class="author">by <strong>elena.mol</strong></div>
				<div class="content">Dear all, <br />I have found problems in SCF runs for “big” systems. My post is related to previous posts regarding “SCF crash when writing WFK” and similar ones, but is a bit more general, since in some cases I have a crash also with prtwf=0.<br /><br />These are the general conclusions I can draw from many tests, on different molecules, different machines (our local cluster, CINECA PLX: <!-- m --><a class="postlink" href="http://www.hpc.cineca.it/hardware/ibm-plx">http://www.hpc.cineca.it/hardware/ibm-plx</a><!-- m -->, and, in the past, CINECA SP6 which is now no longer active) and several choices for parallelization keywords (I always have nkpt=1). I mostly use abinit6.10.3, but people from the CINECA supercomputing center (<!-- m --><a class="postlink" href="http://www.hpc.cineca.it/">http://www.hpc.cineca.it/</a><!-- m -->) have found a similar behaviour with abinit6.12.3, and I found the same problems some time ago with older abinit versions.<br />1) when the system size (determined by  nband, acell, ecut) exceeds some threshold, which depends on the machine one is using, a parallel SCF run, using npband only, will crash when writing the WFK file, i.e. at convergence (or after “nstep” steps), after writing a correct DEN file. <br />2) This problem can be avoided, for a certain range of nband, acell, ecut, by using prtwf=0, and then performing a second run, with prtwf=1, iscf=-2, nstep=1, irdden=1, which reads the DEN file created by the first run, and writes a WFK file. (I hope this procedure is correct also from a “physical” point of view.. does anyone have any suggestions?)<br />3) Increasing  nband and/or acell and/or ecut  further, one reaches a second “threshold” beyond which the “npband only” SCF run will crash also with prtwf=0. In these cases, there are some values of npfft, bandpp which, together with npband, will yield a correctly running calculation. Not all the combinations of npband, npfft, bandpp indicated by abinit as  having an “optimal weight” in a paral_kgb= -n  test run will work, and it is not clear to me how to identify the “correct” sets of npband,npfft,bandpp <br /><br />These crashes happen without any error message directly related to insufficient memory (usually without any clear error message at all), and the calculations, as long as they are running, use quite less than the allocated/available memory.<br /><br />The typical last lines of the log files, for the crashed runs, are of this kind:<br /> ITER STEP NUMBER     1<br /> vtorho : nnsclo_now=  2, note that nnsclo,dbl_nnsclo,istep=  0 0  1<br />rank 3 in job 1  node168ib0_49519   caused collective abort of all ranks<br />  exit status of rank 3: killed by signal 11<br /><br /><br />It seems that, at least in some of the above mentioned cases, where the parallel SCF run is crashing, the corresponding serial calculation can run without problems, but, of course, taking a very long time! (I tried the serial version for very few cases) <br /><br /><br />Does anyone know how to avoid these problems?  <br /><br />Here are the configure options I used for abinit6.10.3. on CINECA PLX:  <br />module purge<br /><br />module load  profile/advanced<br /><br />module load  intel/co-2011.2.137--binary<br /><br />module load  IntelMPI/4.0--binary<br /><br />module load  netcdf/3.6.2--intel--12.1--binary<br /><br />module load  mkl/10.2.2--binary<br /><br /><br />./configure --enable-mpi -enable-mpi-io   --prefix=/gpfs/scratch/userexternal/emolteni/abinit-6.10.3_PLX_MKLlib    FC=mpif90   CC=mpicc  CXX=mpicxx FCFLAGS='-O2' CCFLAGS=' -O2'  --with-netcdf-incs=&quot;-I$NETCDF_INC&quot; --with-netcdf-libs=&quot;-L$NETCDF_LIB -lnetcdf -lnetcdf_c++&quot; --with-trio-flavor=&quot;netcdf&quot;  --with-linalg-libs=&quot;-L$MKL_HOME/lib/em64t  -lmkl_blas95_lp64 -lmkl_lapack95_lp64 -lmkl_scalapack_lp64 -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -lmkl_blacs_intelmpi_lp64 -lpthread -lm&quot; --with-linalg-incs=&quot;-I$MKL_HOME/include/em64t/lp64  -I$MKL_HOME/include&quot; --with-linalg-flavor=&quot;custom&quot; <br /><br /><br />I'm attaching (all from abinit6.10.3 on CINECA PLX):<br />* big_run1.in: input file for a system which works using prtwf=0 and with (at least) the following 2 sets of parall keywords:  npband=5,npfft=4,bandpp=4,    or   npband=10, npfft=4, bandpp=2. <br /><br />* big_run1_crash.log: log file of the crashed run for the same system, but with npband=5, npfft=1, bandpp=4. <br /><br />* very_big_run1.in: input file for a system with acell and nband larger than in big_run1, and for which I have not found yet any set of keywords to avoid the crash.<br /><br />(of course I put nstep=1 in the test runs in order to spare time and cpu resources, since it's clear that the crash, if any, happens at the last SCF step)<br /><br />Thanks a lot in advance<br />cheers<br />Elena<br /><br />Elena Molteni<br />Department of Physics<br />University of Milan<br />via Celoria, 16 <br />I-20133, Milan, Italy<br />and European Theoretical Spectroscopy Facility (ETSF) <br /><!-- m --><a class="postlink" href="http://www.etsf.eu/">http://www.etsf.eu</a><!-- m --></div>
			</div>
			<hr />
			</div>

	<div id="page-footer" class="page-footer">
		<div class="page-number">All times are <span title="Europe/Brussels">UTC+02:00</span><br />Page <strong>1</strong> of <strong>1</strong></div>
			<div class="copyright">
				<p>Powered by <a href="https://www.phpbb.com/">phpBB</a>&reg; Forum Software &copy; phpBB Limited
				</p>
							</div>
	</div>
</div>

</body>

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=9&t=1703&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 03:51:40 GMT -->
</html>
