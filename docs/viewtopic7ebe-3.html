<!DOCTYPE html>
<html dir="ltr" lang="en-gb">

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=3&t=1117&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 03:23:53 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="robots" content="noindex" />

<title>ABINIT Discussion Forums &bull; HPC Abinit</title>

<link href="styles/flat-style/theme/print.css" rel="stylesheet">
<link href="styles/flat-style/theme/bidi.css" rel="stylesheet">
</head>
<body id="phpbb" class="ltr">
<div id="wrap" class="wrap">
	<a id="top" class="top-anchor" accesskey="t"></a>

	<div id="page-header">
		<h1>ABINIT Discussion Forums</h1>
		<p>The meeting place for ABINIT users and developers<br /><a href="index.html">https://forum.abinit.org/</a></p>

		<h2>HPC Abinit</h2>
		<p><a href="viewtopic8671.html?f=3&amp;t=1117">https://forum.abinit.org/viewtopic.php?f=3&amp;t=1117</a></p>
	</div>

	<div id="page-body" class="page-body">
		<div class="page-number">Page <strong>1</strong> of <strong>1</strong></div>
					<div class="post">
				<h3>HPC Abinit</h3>
				<div class="date">Posted: <strong>Wed May 25, 2011 3:58 pm</strong></div>
				<div class="author">by <strong>hpc_sysadmin</strong></div>
				<div class="content">Hi everyone, I'm trying to run abinit 6.6.2 with openMPI 1.4.3 in a cluster where the nodes each have a local filesystem.<br /><br />A problem arises as my user tries to do a multi-dataset case: abinit writes the _DS1_WFK file correctly on the &quot;master&quot; node and an empty _DS1_WFK on the remote nodes. The calculation stops as the program tries to read the first dataset.<br /><br />I tried to run abinit with single-host mpi parallelization and the DSx_WFK files are written correctly and the case completed successfully.<br /><br />I searched the documentation and found a couple of settings that could be relevant to this problem (localrdwf and accesswff); I tried various settings but I could not get abinit to write and read the wavefunction files just from the master node. I also tried to use a centralized networked filesystem but the performance is so awful that I had to discard that possibility (+45% cpu time with 2*cpus against a single-host case).<br /><br />Is there a way to get around this problem? I tried to search the forums but I didn't find anything relevant...</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: HPC Abinit</h3>
				<div class="date">Posted: <strong>Sun Jun 05, 2011 9:32 pm</strong></div>
				<div class="author">by <strong>mverstra</strong></div>
				<div class="content">You are quite correct, this is a pain. The variables do not work systematically for localrdw and so on, and will not do what you are suggesting, which would be to save the WF on all of the processors. The normal possibility would be only 1 copy for the mother node, which reads and broadcasts for everyone, but to my knowledge this is not possible.<br /><br />Centralized NFS disk is indeed too slow in most cases, although for this use it might work: only need i/o at the end or beginning of each dataset. MPI-io would work just as well as direct access if you do have a high performance disk (GPFS or lustre)<br /><br />The simplest for you to do would be to run the DS separately, or make a batch file with several inputs, and in between the DS it scp-s the needed files to the other nodes.<br /><br />abinit&lt; files1 &gt; log1 # first inputs, just for DS1<br />for node in `echo nodefile`<br />do<br />    scp *WFK*  $node:/scratch/$user/<br />done<br />abinit &lt; files2&gt; log2 # rest of the datasets<br /><br />etc... where I presumed /scratch/$user is your local scratch directory.<br />Normally there is just one or a few WF files you need to copy for the rest of the datasets.<br /><br /><br />ciao<br /><br />Matthieu</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: HPC Abinit</h3>
				<div class="date">Posted: <strong>Tue Jun 07, 2011 8:59 pm</strong></div>
				<div class="author">by <strong>maurosgroi</strong></div>
				<div class="content">Dear Matthieu,<br />so, if I understand well, the multidataset mode implemented in abinit cannot be used on a parallel calculation cluster?<br />Is this correct?<br />Is there a way to modify the source in order to write correctly the WFK files on each node?<br />Best regards,<br />Mauro.</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: HPC Abinit</h3>
				<div class="date">Posted: <strong>Wed Jun 08, 2011 12:24 pm</strong></div>
				<div class="author">by <strong>hpc_sysadmin</strong></div>
				<div class="content">First of all, thank you for your feedback. <br /><br /><br /><blockquote><div><cite>mverstra wrote:</cite><br />The simplest for you to do would be to run the DS separately, or make a batch file with several inputs, and in between the DS it scp-s the needed files to the other nodes.<br /></div></blockquote><br /><br />This could work if we have a precise number of datasets to work on, but if we need to do some geometry optimization runs we wouldn't know beforehand how many datasets we'll be working on.<br />Sadly a global cluster filesystem is out of question (our architecture won't permit such a configuration) so our best bet (besides tweaking abinit sources to make it write WFK files on every node, which I guess is way out of my league) would be to mount a shared NFS filesystem on all nodes just for optimization jobs (even though I have goosebumps just thinking about the performances...). <br /><br />Do you think it would be a daunting process to patch the sources in such a fashion? <br /><br />Bye,<br />Dave</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: HPC Abinit</h3>
				<div class="date">Posted: <strong>Wed Oct 12, 2011 11:08 am</strong></div>
				<div class="author">by <strong>mverstra</strong></div>
				<div class="content">Hello Dave,<br /><br />there used to be an option localrdwf to choose between <br />* only mother node reads wf and distributes<br />and<br />* everyone reads their own wf (the present solution)<br /><br />it never worked entirely correctly, and apparently has been disowned, but the code is still there and could easily be used (and hopefully patched) by someone with a little bit of patience. I think it also collided with some of the mpi-io code that was being implemented. Needs to be checked, with a simple test case, but the tough bit will be accounting for all the sub-possibilities of different types of jobs (phonons, ground state, GW, etc...)<br /><br />cheers<br /><br />Matthieu</div>
			</div>
			<hr />
			</div>

	<div id="page-footer" class="page-footer">
		<div class="page-number">All times are <span title="Europe/Brussels">UTC+02:00</span><br />Page <strong>1</strong> of <strong>1</strong></div>
			<div class="copyright">
				<p>Powered by <a href="https://www.phpbb.com/">phpBB</a>&reg; Forum Software &copy; phpBB Limited
				</p>
							</div>
	</div>
</div>

</body>

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=3&t=1117&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 03:23:53 GMT -->
</html>
