<!DOCTYPE html>
<html dir="ltr" lang="en-gb">

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=9&t=356&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 00:19:24 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="robots" content="noindex" />

<title>ABINIT Discussion Forums &bull; mpi-io output for large files</title>

<link href="styles/flat-style/theme/print.css" rel="stylesheet">
<link href="styles/flat-style/theme/bidi.css" rel="stylesheet">
</head>
<body id="phpbb" class="ltr">
<div id="wrap" class="wrap">
	<a id="top" class="top-anchor" accesskey="t"></a>

	<div id="page-header">
		<h1>ABINIT Discussion Forums</h1>
		<p>The meeting place for ABINIT users and developers<br /><a href="index.html">https://forum.abinit.org/</a></p>

		<h2>mpi-io output for large files</h2>
		<p><a href="viewtopic49d2.html?f=9&amp;t=356">https://forum.abinit.org/viewtopic.php?f=9&amp;t=356</a></p>
	</div>

	<div id="page-body" class="page-body">
		<div class="page-number">Page <strong>1</strong> of <strong>1</strong></div>
					<div class="post">
				<h3>mpi-io output for large files</h3>
				<div class="date">Posted: <strong>Wed May 26, 2010 2:08 pm</strong></div>
				<div class="author">by <strong>mverstra</strong></div>
				<div class="content">Hello,<br /><br />I have been running paral_kgb with bands and a bit of fft parallelism for a largeish molecule in a box (limited input file below) on xeon intel 11.1 openmpi 1.3.0 cluster nic3 here in Liege. The speedup is appreciated, even if not linear. I notice that band parallelization does not reduce memory use per core much, whereas fft parallelism does (but it is less efficient, and there is definitely an underestimation in the abinit prediction of memory usage - sometimes by half)<br /><br />I now have a problem in the mpi-io writing of the WFK file for the full case with many bands: I get a segfault inside wffreadwrite_mpio:<br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>&#91;node066:03178&#93; *** Process received signal ***<br />&#91;node066:03178&#93; Signal: Segmentation fault (11)<br />&#91;node066:03178&#93; Signal code: Address not mapped (1)<br />&#91;node066:03178&#93; Failing at address: 0x11891a8c0<br />&#91;node066:03176&#93; *** Process received signal ***<br />&#91;node066:03176&#93; Signal: Segmentation fault (11)<br />&#91;node066:03176&#93; Signal code:&nbsp; (128)<br />&#91;node066:03176&#93; Failing at address: (nil)<br />&#91;node066:03178&#93; &#91; 0&#93; /lib64/libpthread.so.0 &#91;0x3246e0e4c0&#93;<br />&#91;node066:03178&#93; &#91; 1&#93; /cvos/shared/apps/openmpi/intel/64/1.3.0/lib64/openmpi/mca_io_romio.so(ADIOI_Calc_my_req+0x15d) &#91;0x2b2cf916d09d&#93;<br />&#91;node066:03178&#93; &#91; 2&#93; /cvos/shared/apps/openmpi/intel/64/1.3.0/lib64/openmpi/mca_io_romio.so(ADIOI_GEN_WriteStridedColl+0x3f2) &#91;0x2b2cf917fda2&#93;<br />&#91;node066:03178&#93; &#91; 3&#93; /cvos/shared/apps/openmpi/intel/64/1.3.0/lib64/openmpi/mca_io_romio.so(MPIOI_File_write_all+0xc0) &#91;0x2b2cf918a6f0&#93;<br />&#91;node066:03178&#93; &#91; 4&#93; /cvos/shared/apps/openmpi/intel/64/1.3.0/lib64/openmpi/mca_io_romio.so(mca_io_romio_dist_MPI_File_write_all+0x23) &#91;0x2b2cf918a623&#93;<br />&#91;node066:03178&#93; &#91; 5&#93; /cvos/shared/apps/openmpi/intel/64/1.3.0/lib64/openmpi/mca_io_romio.so &#91;0x2b2cf916c7f0&#93;<br />&#91;node066:03178&#93; &#91; 6&#93; /cvos/shared/apps/openmpi/intel/64/1.3.0/lib64/libmpi.so.0(MPI_File_write_all+0x4a) &#91;0x2b2cc9a62e0a&#93;<br />&#91;node066:03178&#93; &#91; 7&#93; /cvos/shared/apps/openmpi/intel/64/1.3.0/lib64/libmpi_f77.so.0(mpi_file_write_all_f+0x8f) &#91;0x2b2cc97e71ff&#93;<br />&#91;node066:03178&#93; &#91; 8&#93; /u/mverstra/CODES/ABINIT/6.1.2-private/tmp-ifort/src/98_main/abinit(wffreadwrite_mpio_+0x16f0) &#91;0x1055380&#93;<br />&#91;node066:03178&#93; &#91; 9&#93; /u/mverstra/CODES/ABINIT/6.1.2-private/tmp-ifort/src/98_main/abinit(writewf_+0x1896) &#91;0x104c3a6&#93;<br />&#91;node066:03178&#93; &#91;10&#93; /u/mverstra/CODES/ABINIT/6.1.2-private/tmp-ifort/src/98_main/abinit(rwwf_+0x8e8) &#91;0x104aaf8&#93;<br />&#91;node066:03178&#93; &#91;11&#93; /u/mverstra/CODES/ABINIT/6.1.2-private/tmp-ifort/src/98_main/abinit(outwf_+0x21cd) &#91;0xc3268d&#93;<br />&#91;node066:03178&#93; &#91;12&#93; /u/mverstra/CODES/ABINIT/6.1.2-private/tmp-ifort/src/98_main/abinit(gstate_+0xc3b0) &#91;0x502250&#93;<br />&#91;node066:03178&#93; &#91;13&#93; /u/mverstra/CODES/ABINIT/6.1.2-private/tmp-ifort/src/98_main/abinit(gstateimg_+0x1409) &#91;0x45bd89&#93;<br />&#91;node066:03178&#93; &#91;14&#93; /u/mverstra/CODES/ABINIT/6.1.2-private/tmp-ifort/src/98_main/abinit(driver_+0x6ddb) &#91;0x451dbb&#93;<br />&#91;node066:03178&#93; &#91;15&#93; /u/mverstra/CODES/ABINIT/6.1.2-private/tmp-ifort/src/98_main/abinit(MAIN__+0x3b07) &#91;0x448057&#93;<br />&#91;node066:03178&#93; &#91;16&#93; /u/mverstra/CODES/ABINIT/6.1.2-private/tmp-ifort/src/98_main/abinit(main+0x3c) &#91;0x44453c&#93;<br />&#91;node066:03178&#93; &#91;17&#93; /lib64/libc.so.6(__libc_start_main+0xf4) &#91;0x324621d974&#93;<br />&#91;node066:03178&#93; &#91;18&#93; /u/mverstra/CODES/ABINIT/6.1.2-private/tmp-ifort/src/98_main/abinit(ctrtri_+0x41) &#91;0x444449&#93;<br /></code></pre></div><br /><br />The density file is written correctly (in mpiio).<br /><br />If I lower ecut or make the box smaller the writing works (and the file can be re-used as input). It looks like it is linked to the file size. <a href="http://lists.mcs.anl.gov/pipermail/mpich-discuss/2007-May/002311.html" class="postlink">http://lists.mcs.anl.gov/pipermail/mpich-discuss/2007-May/002311.html</a> is perhaps related, but for mpich, and I have not found a solution to it.<br /><br />I have tried printing out the size of the buffer arrays in wffreadwrite_mpio:<br /><br />call to MPI_FILE_WRITE_ALL      8947488<br /><br />but they look ok, and the values too. The size of the buffer is not that big (a few hundred Mb), and I did not get an actual complaint about memory allocation, so that is probably not the problem.<br /><br /><br />Does anyone have a suggestion? Has anyone tried openmpi 1.4 with better results? Or would downgrading to 1.2.6 help? I doubt it<br /><br />thanks,<br /><br />Matthieu<br /><br /><br /><blockquote class="uncited"><div>nstep    100          # max iteration scf cicle<br />nband    600           # Number of (occ and empty) bands to be computed<br />paral_kgb 1 <br />npkpt  2<br />npband 10      # Nr of Processors at the BAND level, default 1<br />               # npband has to be a divisor or equal to nband  <br />npfft  3       # Nr of Processors at the FFT level, default nproc<br />               # npfft should be a divisor or equal to the number <br />               # of FFT planes along the 2nd and 3rd dimensions  <br />bandpp 1<br />wfoptalg 4  nloalg 4  fftalg 401  intxc 0  fft_opt_lob 2<br />accesswff 1<br /><br />occopt 3            # 7 = gaussian   4 = cold smearing 3 = FD<br />tsmear 0.01 eV<br /><br />optstress 0<br />optforces 0<br />kptopt 1<br />ngkpt 1 1 1<br />nshiftk 1<br />shiftk 0 0 0<br /><br />ndtset  1<br />tolwfr   1.0d-12<br />nbdbuf   10            # buffer: the last 10 wfns will not converge<br />istwfk    1*1   <br />ixc 1<br />ecut 35<br />nline 6<br />diemac 1<br />diemix 0.7<br />####   Don't use iprcel 45<br /><br />nsppol 2<br />spinat<br />0 0 0.5  <br />0 0 -0.2 <br />...<br />0 0 0<br />0 0 0<br />0 0 0<br />0 0 0.5  <br /><br />acell 17 10 40 Angstr  # 10 angst vacuum<br />rprim 1.0000 0.0000 0.0000<br />      0.0000 1.0000 0.0000<br />      0.0000 0.0000 1.0000<br />natom 59<br />ntypat 2<br />typat ...<br />znucl 6 1<br />xcart<br />...<br /></div></blockquote></div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: mpi-io output for large files</h3>
				<div class="date">Posted: <strong>Wed May 26, 2010 7:26 pm</strong></div>
				<div class="author">by <strong>torrent</strong></div>
				<div class="content">Hi MJ,<br /><br />Yes, you should have a better behaviour with openmpi 1.4.1 !<br />We had a lot of of difficulties with the 1.3 version of openmpi (related to mpi-io)... and they disappear with openmpi 1.4.1.<br />I don't know if it will solve you problem, but try it...<br /><br />The estimation of memory usage within &quot;kgb&quot; parallelization has not been implemented (when you lanch a kgb run , you get the k-point parallelization estimation).<br /><br />Marc</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: mpi-io output for large files</h3>
				<div class="date">Posted: <strong>Tue Jun 01, 2010 1:49 pm</strong></div>
				<div class="author">by <strong>mverstra</strong></div>
				<div class="content">Hi again,<br /><br />no, unfortunately openmpi 1.4.2 segfaults in exactly the same way, when writing the wf file...<br /><br />any other suggestions?<br /><br />Matthieu</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: mpi-io output for large files</h3>
				<div class="date">Posted: <strong>Tue Jun 01, 2010 11:05 pm</strong></div>
				<div class="author">by <strong>mverstra</strong></div>
				<div class="content">Slight progress: ifort with debugging tells me it is trying to access cg_disk while it is not allocated. Unfortunately, the $#^@# compiler is incapable of telling me where and in which routine. Semi useless...<br /><br />Matthieu</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: mpi-io output for large files</h3>
				<div class="date">Posted: <strong>Wed Jun 02, 2010 12:00 pm</strong></div>
				<div class="author">by <strong>torrent</strong></div>
				<div class="content">OK Matthieu,<br /><br />As I'm the one who recently modified (improved), mpi-io access, I feel obliged to help...<br /><br />cg_disk should only be accessed when mkmem=0.<br />Is it the case (I guess no) ?<br /><br />If you &quot;grep&quot; cg_disk in the source files, you find that the only routines concerned with mpi-io are outwf and wfsinp...<br /><br />Marc</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: mpi-io output for large files</h3>
				<div class="date">Posted: <strong>Wed Jun 02, 2010 12:36 pm</strong></div>
				<div class="author">by <strong>torrent</strong></div>
				<div class="content">If I run the automatic MPI-IO tests with -fbounds-check (gcc), everything is correct (no bounds overflow)...<br />??? where is the cg_disk access ?</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: mpi-io output for large files</h3>
				<div class="date">Posted: <strong>Thu Jun 03, 2010 12:10 am</strong></div>
				<div class="author">by <strong>mverstra</strong></div>
				<div class="content">my mistake - the complaint came from adding -C for the compilation - ifort complains because we pass unallocated arrays. We had seen this before. I'm now trying what you mentioned with gfortran fcheck-bounds - ifort with bounds checking is unusable for abinit unfortunately <img class="smilies" src="images/smilies/icon_e_sad.gif" alt=":(" title="Sad" /><br /><br />Matthieu</div>
			</div>
			<hr />
			</div>

	<div id="page-footer" class="page-footer">
		<div class="page-number">All times are <span title="Europe/Brussels">UTC+02:00</span><br />Page <strong>1</strong> of <strong>1</strong></div>
			<div class="copyright">
				<p>Powered by <a href="https://www.phpbb.com/">phpBB</a>&reg; Forum Software &copy; phpBB Limited
				</p>
							</div>
	</div>
</div>

</body>

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=9&t=356&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 00:19:24 GMT -->
</html>
