<!DOCTYPE html>
<html dir="ltr" lang="en-gb">

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=3&t=2884&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 02:51:52 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="robots" content="noindex" />

<title>ABINIT Discussion Forums &bull; help MPI support on  dual CPU server</title>

<link href="styles/flat-style/theme/print.css" rel="stylesheet">
<link href="styles/flat-style/theme/bidi.css" rel="stylesheet">
</head>
<body id="phpbb" class="ltr">
<div id="wrap" class="wrap">
	<a id="top" class="top-anchor" accesskey="t"></a>

	<div id="page-header">
		<h1>ABINIT Discussion Forums</h1>
		<p>The meeting place for ABINIT users and developers<br /><a href="index.html">https://forum.abinit.org/</a></p>

		<h2>help MPI support on  dual CPU server</h2>
		<p><a href="viewtopic77ac.html?f=3&amp;t=2884">https://forum.abinit.org/viewtopic.php?f=3&amp;t=2884</a></p>
	</div>

	<div id="page-body" class="page-body">
		<div class="page-number">Page <strong>1</strong> of <strong>1</strong></div>
					<div class="post">
				<h3>help MPI support on  dual CPU server</h3>
				<div class="date">Posted: <strong>Fri Dec 12, 2014 2:29 am</strong></div>
				<div class="author">by <strong>timber</strong></div>
				<div class="content">hi, I have a lenovo D20 server, dual CPUs and 12 cores.<br />The OS is centos7, with epel repo<br />I configure all the requeirements from vendor and epel repo, and I can run through the tests under serial mode.<br /><br />but when I compile with MPI support, it always failed.   the terminal message is:<br /><br /><br /><br /><br /> ==============================================================================<br /> === Multicore architecture support                                         ===<br /> ==============================================================================<br /><br />checking whether to enable OpenMP support... yes<br />checking Fortran flags for OpenMP... -fopenmp<br />checking whether OpenMP's COLLAPSE works... yes<br />configure: OpenMP support is enabled in Fortran source code only<br />checking whether to build MPI code... yes<br />checking whether the C compiler supports MPI... no<br />checking whether the C++ compiler supports MPI... no<br />checking whether the Fortran Compiler supports MPI... yes<br />checking whether MPI is usable... no<br />configure: error: MPI support is broken - please fix your config parameters and/or MPI installation<br />[root@c64 abinit-7.10.1]# which mpicc <br />/usr/lib64/openmpi/bin/mpicc<br />[root@c64 abinit-7.10.1]# mpicc --version <br />gcc (GCC) 4.8.2 20140120 (Red Hat 4.8.2-16)<br />Copyright (C) 2013 Free Software Foundation, Inc.<br />This is free software; see the source for copying conditions.  There is NO<br />warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.<br /><br /><br /><br /><br /><br /><br />I was confused. if there is something or tricks for a dual CPUs configuration?</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: help MPI support on  dual CPU server</h3>
				<div class="date">Posted: <strong>Fri Dec 12, 2014 8:09 pm</strong></div>
				<div class="author">by <strong>jbeuken</strong></div>
				<div class="content">Hi,<br /><br />two questions and one ideaâ€¦<br /><br />1) what is the content of the file :  /root/.abinit/build/c64.ac<br /><br />2) results of commands :<br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>mpicc -show<br />mpicc --version<br />mpif90 -show<br />mpif90 --version</code></pre></div><br /><br />3) try to put the mpi &quot;compiler&quot; at the beginning of the PATH variable :<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>export PATH=/usr/lib64/openmpi/bin:$PATH<br />export LD_LIBRARY_PATH=/usr/lib64/openmpi/lib:$LD_LIBRARY_PATH<br />./configure --enable-mpi --enable-mpi-io <br /></code></pre></div><br /><br />jmb</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: help MPI support on  dual CPU server</h3>
				<div class="date">Posted: <strong>Sat Dec 13, 2014 4:19 am</strong></div>
				<div class="author">by <strong>timber</strong></div>
				<div class="content">checking whether to use C clock for timings... no<br /><br /> ==============================================================================<br /> === Multicore architecture support                                         ===<br /> ==============================================================================<br /><br />checking whether to enable OpenMP support... yes<br />checking Fortran flags for OpenMP... -fopenmp<br />checking whether OpenMP's COLLAPSE works... yes<br />configure: OpenMP support is enabled in Fortran source code only<br />checking whether to build MPI code... yes<br />checking whether the C compiler supports MPI... no<br />checking whether the C++ compiler supports MPI... no<br />checking whether the Fortran Compiler supports MPI... yes<br />checking whether MPI is usable... no<br />configure: error: MPI support is broken - please fix your config parameters and/or MPI installation<br /><br />[root@c64 abinit-7.10.1]# mpicc --show <br />gcc -I/usr/include/openmpi-x86_64 -pthread -m64 -L/usr/lib64/openmpi/lib -lmpi<br />[root@c64 abinit-7.10.1]# mpicc --version<br />gcc (GCC) 4.8.2 20140120 (Red Hat 4.8.2-16)<br />Copyright (C) 2013 Free Software Foundation, Inc.<br />This is free software; see the source for copying conditions.  There is NO<br />warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.<br /><br />[root@c64 abinit-7.10.1]# mpif90 --show <br />gfortran -I/usr/include/openmpi-x86_64 -pthread -m64 -I/usr/lib64/openmpi/lib -L/usr/lib64/openmpi/lib -lmpi_f90 -lmpi_f77 -lmpi<br />[root@c64 abinit-7.10.1]# mpif90 --version<br />GNU Fortran (GCC) 4.8.2 20140120 (Red Hat 4.8.2-16)<br />Copyright (C) 2013 Free Software Foundation, Inc.<br /><br />GNU Fortran comes with NO WARRANTY, to the extent permitted by law.<br />You may redistribute copies of GNU Fortran<br />under the terms of the GNU General Public License.<br />For more information about these matters, see the file named COPYING<br /><br />[root@c64 abinit-7.10.1]# export |grep -i path<br />declare -x LD_LIBRARY_PATH=&quot;/usr/lib64/openmpi/lib:&quot;<br />declare -x MODULEPATH=&quot;/usr/share/Modules/modulefiles:/etc/modulefiles&quot;<br />declare -x PATH=&quot;/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin:/sbin:/usr/lib64/openmpi/bin&quot;<br />declare -x WINDOWPATH=&quot;1&quot;</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: help MPI support on  dual CPU server</h3>
				<div class="date">Posted: <strong>Sat Dec 13, 2014 4:22 am</strong></div>
				<div class="author">by <strong>timber</strong></div>
				<div class="content">the configuration file for centos 64 <br />         run  under vmware workstation 10<br /><br />I have tried run another cenos7 , in real mode.   The message is the same, corrupt  on  mpicc<br />The host OS is windows server 2012, I have to shared it with  other colleagues.</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: help MPI support on  dual CPU server</h3>
				<div class="date">Posted: <strong>Sat Dec 13, 2014 10:53 pm</strong></div>
				<div class="author">by <strong>jbeuken</strong></div>
				<div class="content">Hi,<br /><br />I found a &quot;little&quot; error in the c64.ac file<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>CC = &quot;/usr/lib64/openmpi/bin/mpicc<br /></code></pre></div><br />instead of <br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>CC=&quot;/usr/lib64/openmpi/bin/mpicc&quot;</code></pre></div><br /><br />I configured a machine under CentOS 7 with openmpi/ffw3  and with a  .ac file  ( without some fallbacks in the first time ) :<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>enable_64bit_flags=&quot;yes&quot;<br />enable_debug=&quot;yes&quot;<br />prefix=&quot;/opt/abinit4.10gcc48openMp&quot;<br />CC=&quot;/usr/lib64/openmpi/bin/mpicc&quot;<br />FC=&quot;mpif90&quot;<br />enable_mpi=&quot;yes&quot;<br />enable_mpi_io=&quot;yes&quot;<br />with_mpi_incs=&quot;-I/usr/include/openmpi-x86_64&quot;<br />with_mpi_libs=&quot;-L/usr/lib64/openmpi/lib -lmpi&quot;<br />with_fft_libs=&quot;-L/usr/lib64/ -lfftw3 -lfftw3f&quot;<br />with_dft_flavor=&quot;libxc&quot;<br />with_trio_flavor=&quot;none&quot;<br />enable_fallbacks=&quot;yes&quot;<br />enable_openmp=&quot;yes&quot;<br /></code></pre></div><br />and with a environment variables  ( part ) :<br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>PATH=/usr/lib64/openmpi/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:~/bin<br />LD_LIBRARY_PATH=/usr/lib64/openmpi/lib:</code></pre></div><br /><br />the &quot;./configure&quot; and &quot;make mj4&quot; work  !<br /><br />jmb</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: help MPI support on  dual CPU server</h3>
				<div class="date">Posted: <strong>Sun Dec 14, 2014 9:03 am</strong></div>
				<div class="author">by <strong>timber</strong></div>
				<div class="content">I revised the configuration file. but the script output the same error message.<br />I think I had use this setting before.<br /><br />config.log is attached.<br /><br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>==============================================================================<br />&nbsp;=== Multicore architecture support&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;===<br />&nbsp;==============================================================================<br /><br />checking whether to enable OpenMP support... yes<br />checking Fortran flags for OpenMP... -fopenmp<br />checking whether OpenMP's COLLAPSE works... yes<br />configure: OpenMP support is enabled in Fortran source code only<br />checking whether to build MPI code... yes<br />checking whether the C compiler supports MPI... no<br />checking whether the C++ compiler supports MPI... no<br />checking whether the Fortran Compiler supports MPI... yes<br />checking whether MPI is usable... no<br />configure: error: MPI support is broken - please fix your config parameters and/or MPI installation<br />&#91;root@c64 abinit-7.10.1&#93;# grep&nbsp; mpicc config.log <br />configure:9788: checking for mpicc<br />configure:9806: found /usr/lib64/openmpi/bin/mpicc<br />configure:9818: result: /usr/lib64/openmpi/bin/mpicc<br />configure:9908: result: mpicc<br />configure:10148: mpicc --version &gt;&amp;5<br />configure:10159: mpicc -v &gt;&amp;5<br />configure:10170: mpicc -V &gt;&amp;5<br />configure:10219: mpicc&nbsp; &nbsp; conftest.c&nbsp; &gt;&amp;5<br />configure:10332: mpicc -o conftest&nbsp; &nbsp; conftest.c&nbsp; &gt;&amp;5<br />configure:10394: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:10456: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:10489: checking whether mpicc accepts -g<br />configure:10519: mpicc -c -g&nbsp; conftest.c &gt;&amp;5<br />configure:10644: checking for mpicc option to accept ISO C89<br />configure:10718: mpicc&nbsp; -c -g -O2&nbsp; conftest.c &gt;&amp;5<br />configure:10830: checking dependency style of mpicc<br />configure:11056: mpicc -E&nbsp; conftest.c<br />configure:11094: mpicc -E&nbsp; conftest.c<br />configure:11134: result: mpicc -E<br />configure:11163: mpicc -E&nbsp; conftest.c<br />configure:11201: mpicc -E&nbsp; conftest.c<br />configure:11541: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11647: mpicc -o conftest&nbsp; &nbsp; conftest.c&nbsp; &gt;&amp;5<br />configure:11724: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11724: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11724: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11724: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11724: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11724: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11724: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11724: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11724: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11788: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11845: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:11884: mpicc -c&nbsp; &nbsp;conftest.c &gt;&amp;5<br />configure:14907: mpicc -o conftest&nbsp; &nbsp; conftest.c&nbsp; &nbsp;-L/usr/lib64/openmpi/lib -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../../../lib64 -L/lib/../lib64 -L/usr/lib/../lib64 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../.. -lmpi_f90 -lmpi_f77 -lmpi -lgfortran -lm -lquadmath -lpthread &gt;&amp;5<br />configure:15122: mpicc -o conftest&nbsp; &nbsp; conftest.c cfortran_test.o&nbsp; &nbsp;-L/usr/lib64/openmpi/lib -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../../../lib64 -L/lib/../lib64 -L/usr/lib/../lib64 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../.. -lmpi_f90 -lmpi_f77 -lmpi -lgfortran -lm -lquadmath -lpthread &gt;&amp;5<br />configure:15122: mpicc -o conftest&nbsp; &nbsp; conftest.c cfortran_test.o&nbsp; &nbsp;-L/usr/lib64/openmpi/lib -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../../../lib64 -L/lib/../lib64 -L/usr/lib/../lib64 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../.. -lmpi_f90 -lmpi_f77 -lmpi -lgfortran -lm -lquadmath -lpthread &gt;&amp;5<br />configure:15211: mpicc -o conftest&nbsp; &nbsp; conftest.c cfortran_test.o&nbsp; &nbsp;-L/usr/lib64/openmpi/lib -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../../../lib64 -L/lib/../lib64 -L/usr/lib/../lib64 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.2/../../.. -lmpi_f90 -lmpi_f77 -lmpi -lgfortran -lm -lquadmath -lpthread &gt;&amp;5<br />configure:15560: mpicc -c&nbsp; -I/usr/include/python2.7 -I/usr/lib64/python2.7/site-packages/numpy/core/include&nbsp; conftest.c &gt;&amp;5<br />configure:15600: mpicc -E -I/usr/include/python2.7 -I/usr/lib64/python2.7/site-packages/numpy/core/include&nbsp; conftest.c<br />configure:15718: mpicc -c&nbsp; -I/usr/include/python2.7 -I/usr/lib64/python2.7/site-packages/numpy/core/include&nbsp; conftest.c &gt;&amp;5<br />configure:21237: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:21277: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:21237: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:21277: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:21390: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:21430: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:21390: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:21430: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:21390: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:21430: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:21541: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:21581: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:21693: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:21733: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:21693: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:21733: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:21844: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:21884: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:21995: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:22035: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:22146: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:22186: mpicc -E&nbsp; &nbsp; &nbsp; conftest.c<br />configure:22336: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:22445: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:22847: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:23254: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:23661: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:24068: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:24475: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:24882: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:25289: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:25696: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:26103: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:26510: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:26917: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:27324: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:27731: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;conftest.c&nbsp; &gt;&amp;5<br />configure:27865: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:27933: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:27975: mpicc -m64 -c -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; conftest.c &gt;&amp;5<br />configure:30260: mpicc -m64 -o conftest -m64&nbsp; -O2 -mtune=native -march=native&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; conftest.c&nbsp; &nbsp; &nbsp; &nbsp;&gt;&amp;5<br />ac_cv_path_abi_cc_path=/usr/lib64/openmpi/bin/mpicc<br />ac_cv_prog_CPP='mpicc -E'<br />ac_cv_prog_ac_ct_CC=mpicc<br />CC='mpicc -m64'<br />CPP='mpicc -E'<br />abi_cc_path='/usr/lib64/openmpi/bin/mpicc'<br />ac_ct_CC='mpicc'<br /><br /><br /></code></pre></div></div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: help MPI support on  dual CPU server</h3>
				<div class="date">Posted: <strong>Mon Dec 15, 2014 2:36 am</strong></div>
				<div class="author">by <strong>timber</strong></div>
				<div class="content">ok, I just tried another configuration,  with the intel compliler 2013<br /><br />I compile the openmpi with options <br />./configure CC=icc  CXX=icpc FC=ifort --prefix=/opt/openmpi-intel/<br />after make, I enter  make check, the result show all tests passed.<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>Contiguous multiple data-type (1*4500)<br />raw extraction in 0 microsec<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />Vector data-type (450 times 10 double stride 11)<br />raw extraction in 8 microsec<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />raw extraction in 131 microsec<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />raw extraction in 138 microsec<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />raw extraction in 955 microsec<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />raw extraction in 0 microsec<br />&gt;&gt;--------------------------------------------&lt;&lt;<br />PASS: ddt_raw<br />==================<br />All 6 tests passed<br />==================<br />make&#91;3&#93;: Leaving directory `/root/openmpi-1.8.3/test/datatype'<br />make&#91;2&#93;: Leaving directory `/root/openmpi-1.8.3/test/datatype'<br />Making check in util<br />make&#91;2&#93;: Entering directory `/root/openmpi-1.8.3/test/util'<br />make&nbsp; opal_bit_ops opal_path_nfs<br />make&#91;3&#93;: Entering directory `/root/openmpi-1.8.3/test/util'<br />make&#91;3&#93;: `opal_bit_ops' is up to date.<br />make&#91;3&#93;: `opal_path_nfs' is up to date.<br />make&#91;3&#93;: Leaving directory `/root/openmpi-1.8.3/test/util'<br />make&nbsp; check-TESTS<br />make&#91;3&#93;: Entering directory `/root/openmpi-1.8.3/test/util'<br />SUPPORT: OMPI Test Passed: opal_bit_ops(): (70 tests)<br />PASS: opal_bit_ops<br />SUPPORT: OMPI Test Passed: opal_path_nfs(): (30 tests)<br />PASS: opal_path_nfs<br />==================<br />All 2 tests passed<br />==================<br />make&#91;3&#93;: Leaving directory `/root/openmpi-1.8.3/test/util'<br />make&#91;2&#93;: Leaving directory `/root/openmpi-1.8.3/test/util'<br />make&#91;2&#93;: Entering directory `/root/openmpi-1.8.3/test'<br />make&#91;2&#93;: Nothing to be done for `check-am'.<br />make&#91;2&#93;: Leaving directory `/root/openmpi-1.8.3/test'<br />make&#91;1&#93;: Leaving directory `/root/openmpi-1.8.3/test'<br />make&#91;1&#93;: Entering directory `/root/openmpi-1.8.3'<br />make&#91;1&#93;: Nothing to be done for `check-am'.<br />make&#91;1&#93;: Leaving directory `/root/openmpi-1.8.3'<br /></code></pre></div><br /><br />then compiler infomation<br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>&#91;root@c64 abinit-7.10.1&#93;# mpicc --show <br />icc -I/opt/openmpi-intel/include -pthread -Wl,-rpath -Wl,/opt/openmpi-intel/lib -Wl,--enable-new-dtags -L/opt/openmpi-intel/lib -lmpi<br />&#91;root@c64 abinit-7.10.1&#93;# mpiCC --show <br />icpc -I/opt/openmpi-intel/include -pthread -Wl,-rpath -Wl,/opt/openmpi-intel/lib -Wl,--enable-new-dtags -L/opt/openmpi-intel/lib -lmpi_cxx -lmpi<br />&#91;root@c64 abinit-7.10.1&#93;# mpiCC --version<br />icpc (ICC) 14.0.2 20140120<br />Copyright (C) 1985-2014 Intel Corporation.&nbsp; All rights reserved.<br /><br />&#91;root@c64 abinit-7.10.1&#93;# mpicc --version<br />icc (ICC) 14.0.2 20140120<br />Copyright (C) 1985-2014 Intel Corporation.&nbsp; All rights reserved.<br /><br /></code></pre></div><br /><br /><br />the error message is the same, fortran support, but  c and cpp are failed.<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>checking whether the Fortran compiler accepts the PROTECTED attribute... yes<br />checking whether the Fortran compiler supports stream IO... yes<br />checking whether the Fortran compiler accepts cpu_time()... yes<br />checking whether the Fortran compiler accepts etime()... no<br />checking whether to use C clock for timings... no<br /><br />&nbsp;==============================================================================<br />&nbsp;=== Multicore architecture support&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;===<br />&nbsp;==============================================================================<br /><br />checking whether to enable OpenMP support... yes<br />checking Fortran flags for OpenMP... -openmp<br />checking whether OpenMP's COLLAPSE works... yes<br />configure: OpenMP support is enabled in Fortran source code only<br />checking whether to build MPI code... yes<br />checking whether the C compiler supports MPI... no<br />checking whether the C++ compiler supports MPI... no<br />checking whether the Fortran Compiler supports MPI... yes<br />checking whether MPI is usable... no<br />configure: error: MPI support is broken - please fix your config parameters and/or MPI installation<br /></code></pre></div><br /><br />config.log is attached</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: help MPI support on  dual CPU server</h3>
				<div class="date">Posted: <strong>Mon Dec 15, 2014 2:39 am</strong></div>
				<div class="author">by <strong>timber</strong></div>
				<div class="content">attachment is the config.log of openompi  with intel compiler<br /><br />SO the openmpi  works.     <br /><br />but why the detection in abinit  failed<br /><br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>&#91;root@c64 openmpi-1.8.3&#93;# ldconfig&nbsp; --print&nbsp; |grep -i mpi&nbsp; <br />&nbsp; &nbsp;libvt.so.0 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libvt.so.0<br />&nbsp; &nbsp;libvt.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libvt.so<br />&nbsp; &nbsp;libvt-mt.so.0 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libvt-mt.so.0<br />&nbsp; &nbsp;libvt-mt.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libvt-mt.so<br />&nbsp; &nbsp;libvt-mpi.so.0 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libvt-mpi.so.0<br />&nbsp; &nbsp;libvt-mpi.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libvt-mpi.so<br />&nbsp; &nbsp;libvt-mpi-unify.so.0 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libvt-mpi-unify.so.0<br />&nbsp; &nbsp;libvt-mpi-unify.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libvt-mpi-unify.so<br />&nbsp; &nbsp;libvt-hyb.so.0 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libvt-hyb.so.0<br />&nbsp; &nbsp;libvt-hyb.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libvt-hyb.so<br />&nbsp; &nbsp;libotfaux.so.0 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libotfaux.so.0<br />&nbsp; &nbsp;libotfaux.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libotfaux.so<br />&nbsp; &nbsp;liboshmem.so.3 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/liboshmem.so.3<br />&nbsp; &nbsp;liboshmem.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/liboshmem.so<br />&nbsp; &nbsp;libopen-trace-format.so.1 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libopen-trace-format.so.1<br />&nbsp; &nbsp;libopen-trace-format.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libopen-trace-format.so<br />&nbsp; &nbsp;libopen-rte.so.7 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libopen-rte.so.7<br />&nbsp; &nbsp;libopen-rte.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libopen-rte.so<br />&nbsp; &nbsp;libopen-pal.so.6 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libopen-pal.so.6<br />&nbsp; &nbsp;libopen-pal.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libopen-pal.so<br />&nbsp; &nbsp;libompitrace.so.0 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libompitrace.so.0<br />&nbsp; &nbsp;libompitrace.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libompitrace.so<br />&nbsp; &nbsp;libmpi_usempif08.so.0 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmpi_usempif08.so.0<br />&nbsp; &nbsp;libmpi_usempif08.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmpi_usempif08.so<br />&nbsp; &nbsp;libmpi_usempi_ignore_tkr.so.0 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmpi_usempi_ignore_tkr.so.0<br />&nbsp; &nbsp;libmpi_usempi_ignore_tkr.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmpi_usempi_ignore_tkr.so<br />&nbsp; &nbsp;libmpi_mpifh.so.2 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmpi_mpifh.so.2<br />&nbsp; &nbsp;libmpi_mpifh.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmpi_mpifh.so<br />&nbsp; &nbsp;libmpi_cxx.so.1 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmpi_cxx.so.1<br />&nbsp; &nbsp;libmpi_cxx.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmpi_cxx.so<br />&nbsp; &nbsp;libmpi.so.1 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmpi.so.1<br />&nbsp; &nbsp;libmpi.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmpi.so<br />&nbsp; &nbsp;libmca_common_sm.so.4 (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmca_common_sm.so.4<br />&nbsp; &nbsp;libmca_common_sm.so (libc6,x86-64) =&gt; /opt/openmpi-intel/lib/libmca_common_sm.so<br />&nbsp; &nbsp;libexempi.so.3 (libc6,x86-64) =&gt; /lib64/libexempi.so.3<br /></code></pre></div></div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: help MPI support on  dual CPU server</h3>
				<div class="date">Posted: <strong>Mon Dec 15, 2014 8:16 pm</strong></div>
				<div class="author">by <strong>jbeuken</strong></div>
				<div class="content">very difficult to understand  <img class="smilies" src="images/smilies/icon_e_confused.gif" alt=":?" title="Confused" /> <br />can you send me the result of : env<br />jmb</div>
			</div>
			<hr />
			</div>

	<div id="page-footer" class="page-footer">
		<div class="page-number">All times are <span title="Europe/Brussels">UTC+02:00</span><br />Page <strong>1</strong> of <strong>1</strong></div>
			<div class="copyright">
				<p>Powered by <a href="https://www.phpbb.com/">phpBB</a>&reg; Forum Software &copy; phpBB Limited
				</p>
							</div>
	</div>
</div>

</body>

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=3&t=2884&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 02:51:52 GMT -->
</html>
