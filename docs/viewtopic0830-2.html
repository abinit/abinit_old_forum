<!DOCTYPE html>
<html dir="ltr" lang="en-gb">

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=3&t=4057&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 20 Sep 2024 18:32:37 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="robots" content="noindex" />

<title>ABINIT Discussion Forums &bull; Compiling with MPI and Intel 19.0</title>

<link href="styles/flat-style/theme/print.css" rel="stylesheet">
<link href="styles/flat-style/theme/bidi.css" rel="stylesheet">
</head>
<body id="phpbb" class="ltr">
<div id="wrap" class="wrap">
	<a id="top" class="top-anchor" accesskey="t"></a>

	<div id="page-header">
		<h1>ABINIT Discussion Forums</h1>
		<p>The meeting place for ABINIT users and developers<br /><a href="index.html">https://forum.abinit.org/</a></p>

		<h2>Compiling with MPI and Intel 19.0</h2>
		<p><a href="viewtopic23a5.html?f=3&amp;t=4057">https://forum.abinit.org/viewtopic.php?f=3&amp;t=4057</a></p>
	</div>

	<div id="page-body" class="page-body">
		<div class="page-number">Page <strong>1</strong> of <strong>1</strong></div>
					<div class="post">
				<h3>Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Sun Mar 03, 2019 11:13 pm</strong></div>
				<div class="author">by <strong>frodo</strong></div>
				<div class="content">Hello,<br /><br />I'm having trouble running the abinit (8.10.2) executable after compiling with the Intel 19.0 compilers and with MPI enabled (64 bit intel).<br /><br />If I compile with either the gnu tools (gcc, gfortran 7.3.0) or the Intel tools (icc, ifort), and without MPI enabled, make check shows all fast tests succeed.<br /><br />If I compile with the gnu tools (gcc, gfortran) and MPI enabled, make check runs abinit without mpirun and all fast tests succeed.<br /><br />If I compile with the Intel tools (mpiicc, mpiifort) and MPI enabled, make check still runs abinit without mpirun and all fast tests fail with the following error:<br /><br />forrtl: severe (24): end-of-file during read, unit 5, file /proc/19230/fd/0<br />Image              PC                Routine            Line        Source             <br />libifcoremt.so.5   000014A9FFCA97B6  for__io_return        Unknown  Unknown<br />libifcoremt.so.5   000014A9FFCE7C00  for_read_seq_fmt      Unknown  Unknown<br />abinit             00000000015B9312  Unknown               Unknown  Unknown<br />abinit             0000000000409DEF  Unknown               Unknown  Unknown<br />abinit             0000000000409B22  Unknown               Unknown  Unknown<br />libc-2.27.so       000014A9FD6E1B97  __libc_start_main     Unknown  Unknown<br />abinit             0000000000409A0A  Unknown               Unknown  Unknown<br /><br />If I compile with the Intel tools and MPI enabled and run &quot;runtests.py fast --force-mpirun&quot;, then abinit is run with &quot;mpirun -np 1&quot; and all tests succeed.<br /><br />My understanding is that executables compiled with mpiifort must be run with mpirun even if np=1.<br /><br />It seems that runtests.py tries to run serial tests without mpirun. This seems to work when abinit is compiled with the gnu tools, but not when compiled with the intel tools.<br /><br />Is this a known difference in behavior for mpi executables compiled with the gnu tools vs the intel tools? If so, why doesn't runtests.py use mpirun for the intel compiled executable even for serial tests. Or am I doing something wrong?<br /><br />Also, when compiling with Intel and MPI, setting with_mpi_incs and with_mpi_libs has no effect. They are not used in the compilation. I assume this is because mpiifort is a wrapper that is supplying these. Is that correct?<br /><br />I am using Intel Parallel Studio XE and source psxevars.sh to set the environment before compiling/running with intel.<br /><br />Thanks for any suggestions.</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Thu Mar 07, 2019 5:24 pm</strong></div>
				<div class="author">by <strong>ebousquet</strong></div>
				<div class="content">Dear Frodo,<br />You can type &quot;./runtests.py -h&quot; to see all the available options.<br />For the parallel tests, you have the option runtests.py paral -n XX with XX the number of CPU on which you want to run the mpi. See the other options to force some executable, etc.<br />Best wishes,<br />Eric</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Thu Mar 07, 2019 7:52 pm</strong></div>
				<div class="author">by <strong>frodo</strong></div>
				<div class="content">Hi Eric,<br /><br />Thank you for the reply.<br /><br />Yes, I know about runtests.py and use it all the time.<br /><br />My question was why running &quot;make check&quot; fails with the indicated error when abinit is compiled with Intel 19.0 (mpiifort), where &quot;make check&quot; does not fail when abinit is compiled with gnu (mpif90). In both cases, &quot;make check&quot; invokes runtests.py to run abinit directly, i.e., without mpirun. This works for the gnu compiled version but fails with the error I indicated for the Intel compiled version.<br /><br />Let me try to make what I am saying more clear:<br /><br /><strong class="text-strong">Compiling with GNU:</strong><br /><br />I compile abinit with the following config.ac:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>enable_debug=&quot;no&quot;<br />enable_avx_safe_mode=&quot;no&quot;<br />prefix=&quot;/usr/local/abinit&quot;<br />enable_mpi=&quot;yes&quot;<br />enable_mpi_inplace=&quot;yes&quot;<br />enable_mpi_io=&quot;yes&quot;<br />with_mpi_prefix=&quot;/usr&quot;<br />enable_gpu=&quot;no&quot;</code></pre></div><br /><br />Configure automatically sets CC=mpicc and FC=mpif90.<br /><br />I run &quot;abinit &lt; test.stdin &gt; test.sdtout 2&gt; test.stderr&quot;. This executes normally.<br /><br /><strong class="text-strong">Compiling with Intel 19.0:</strong><br /><br />I compile abinit with the following config.ac:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>enable_debug=&quot;no&quot;<br />enable_avx_safe_mode=&quot;no&quot;<br />prefix=&quot;/usr/local/abinit&quot;<br />CC=&quot;mpiicc&quot;<br />CXX=&quot;mpiicpc&quot;<br />FC=&quot;mpiifort&quot;<br />enable_mpi=&quot;yes&quot;<br />enable_mpi_inplace=&quot;yes&quot;<br />enable_mpi_io=&quot;yes&quot;<br />enable_gpu=&quot;no&quot;</code></pre></div><br /><br />I run &quot;abinit &lt; test.stdin &gt; test.stdout 2&gt; test.stderr&quot;. This fails with the following error:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>forrtl: severe (24): end-of-file during read, unit 5, file /proc/19230/fd/0<br />Image PC Routine Line Source<br />libifcoremt.so.5 000014A9FFCA97B6 for__io_return Unknown Unknown<br />libifcoremt.so.5 000014A9FFCE7C00 for_read_seq_fmt Unknown Unknown<br />abinit 00000000015B9312 Unknown Unknown Unknown<br />abinit 0000000000409DEF Unknown Unknown Unknown<br />abinit 0000000000409B22 Unknown Unknown Unknown<br />libc-2.27.so 000014A9FD6E1B97 __libc_start_main Unknown Unknown<br />abinit 0000000000409A0A Unknown Unknown Unknown<br /></code></pre></div><br /><br />However, if I run &quot;mpirun -np 1 abinit &lt; test.stdin &gt; test.stdout 2&gt; test.stderr&quot;, it execute normally.<br /><br />In other words, the intel compiled version has to be executed with mpirun, even when np=1. But the gnu compiled version can be executed directly, without mpirun.<br /><br />I know I can force runtests.py to use mpirun to invoke abinit (--force-mpirun) but why do I have to do this even for np=1 when I am testing an intel compiled executabe but I do not have to do this (i.e., don't have to use --force-mpirun) for a gnu compiled executable?<br /><br />What is the reason for this difference in behavior?</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Fri Mar 08, 2019 11:07 am</strong></div>
				<div class="author">by <strong>jbeuken</strong></div>
				<div class="content">Hi,<br /><br />quickly: I don't reproduce the behavior you observe  <img class="smilies" src="images/smilies/icon_rolleyes.gif" alt=":roll:" title="Rolling Eyes" /> <br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>&#91;root@yquem fast_t01&#93;# mpiifort -V<br />Intel(R) Fortran Intel(R) 64 Compiler for applications running on Intel(R) 64, Version 17.0.4.196 Build 20170411<br />Copyright (C) 1985-2017 Intel Corporation.&nbsp; All rights reserved.<br />FOR NON-COMMERCIAL USE ONLY</code></pre></div><br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>../../../src/98_main/abinit &lt; t01.stdin &gt; OUT<br /><br />tail t01.out<br />- Comment : the original paper describing the ABINIT project.<br />- DOI and bibtex : see https://docs.abinit.org/theory/bibliography/#gonze2002<br />-<br />- Proc.&nbsp; &nbsp;0 individual time (sec): cpu=&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.1&nbsp; wall=&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.1<br /><br />================================================================================<br /><br />&nbsp;Calculation completed.<br />.Delivered&nbsp; &nbsp;6 WARNINGs and&nbsp; 10 COMMENTs to log file.<br />+Overall time at end (sec) : cpu=&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.1&nbsp; wall=&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.1</code></pre></div><br /><br /><br />my .ac file :<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>CC=&quot;mpiicc&quot;<br />CXX=&quot;mpiicpc&quot;<br />FC=&quot;mpiifort&quot;<br />FCFLAGS_EXTRA=&quot;-g -O3 -align all&quot;<br /><br />enable_mpi=&quot;yes&quot;<br />enable_mpi_inplace=&quot;yes&quot;<br />enable_mpi_io=&quot;yes&quot;<br />with_trio_flavor=none<br />with_dft_flavor=none<br /><br />#I_MPI_ROOT=/opt/intel/compilers_and_libraries_2017.4.196/linux/mpi/<br />with_mpi_incs=&quot;-I${I_MPI_ROOT}/include64&quot;<br />with_mpi_libs=&quot;-L${I_MPI_ROOT}/lib64 -lmpi&quot;<br /><br />with_fft_flavor=&quot;fftw3-mkl&quot;<br />with_fft_incs=&quot;-I${MKLROOT}/include&quot;<br />with_fft_libs=&quot;-L${MKLROOT}/lib/intel64 -Wl,--start-group&nbsp; -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -Wl,--end-group -lpthread -lm -ldl&quot;<br />with_linalg_flavor=&quot;mkl&quot;<br />with_linalg_incs=&quot;-I${MKLROOT}/include&quot;<br />with_linalg_libs=&quot;-L${MKLROOT}/lib/intel64 -Wl,--start-group&nbsp; -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -Wl,--end-group -lpthread -lm -ldl&quot;</code></pre></div><br /><br />jmb</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Sun Mar 10, 2019 7:02 pm</strong></div>
				<div class="author">by <strong>frodo</strong></div>
				<div class="content">Hi,<br /><br />Thanks for the reply.<br /><br />My mpiifort:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>mpiifort -V<br />Intel(R) Fortran Intel(R) 64 Compiler for applications running on Intel(R) 64, Version 19.0.1.144 Build 20181018<br />Copyright (C) 1985-2018 Intel Corporation.&nbsp; All rights reserved.</code></pre></div><br /><br /><br />I recompiled using exactly your config.ac <br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>CC=&quot;mpiicc&quot;<br />CXX=&quot;mpiicpc&quot;<br />FC=&quot;mpiifort&quot;<br />FCFLAGS_EXTRA=&quot;-g -O3 -align all&quot;<br /><br />enable_mpi=&quot;yes&quot;<br />enable_mpi_inplace=&quot;yes&quot;<br />enable_mpi_io=&quot;yes&quot;<br />with_trio_flavor=none<br />with_dft_flavor=none<br /><br />#I_MPI_ROOT=/opt/intel/compilers_and_libraries_2017.4.196/linux/mpi/<br />with_mpi_incs=&quot;-I${I_MPI_ROOT}/include64&quot;<br />with_mpi_libs=&quot;-L${I_MPI_ROOT}/lib64 -lmpi&quot;<br /><br />with_fft_flavor=&quot;fftw3-mkl&quot;<br />with_fft_incs=&quot;-I${MKLROOT}/include&quot;<br />with_fft_libs=&quot;-L${MKLROOT}/lib/intel64 -Wl,--start-group&nbsp; -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -Wl,--end-group -lpthread -lm -ldl&quot;<br />with_linalg_flavor=&quot;mkl&quot;<br />with_linalg_incs=&quot;-I${MKLROOT}/include&quot;<br />with_linalg_libs=&quot;-L${MKLROOT}/lib/intel64 -Wl,--start-group&nbsp; -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -Wl,--end-group -lpthread -lm -ldl&quot;<br /></code></pre></div><br /><br />A comment on this is that apparently Intel 19 changed the library structure for MPI. MPI Libs are in $[I_MPI_ROOT]/intel64/lib, not ${I_MPI_ROOT}lib64. Also, the mpi library is now in a subdirectory of this: ${I_MPI_ROOT}/intel64/lib/release. See: <a href="https://github.com/spack/spack/issues/9913" class="postlink">https://github.com/spack/spack/issues/9913</a><br /><br />Also, with_mpi_incs and with_mpi_libs are not used. Here is the actual compilation line emitted by make:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>mpiifort -DHAVE_CONFIG_H -I. -I../../../src/98_main -I../..&nbsp; -I../../src/incs -I../../../src/incs -I/home/dierker/abinit-8.10.2/build/fallbacks/exports/include&nbsp; -I/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include -I/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include&nbsp; &nbsp;-free -module /home/dierker/abinit-8.10.2/build/src/mods&nbsp; &nbsp;-O3 -g -extend-source -noaltparam -nofpscomp -g -O3 -align all&nbsp; &nbsp;-g -extend-source -noaltparam -nofpscomp -g -O3 -align all&nbsp; -c -o abinit-abinit.o `test -f 'abinit.F90' || echo '../../../src/98_main/'`abinit.F90<br />mpiifort -free -module /home/dierker/abinit-8.10.2/build/src/mods&nbsp; &nbsp;-O3 -g -extend-source -noaltparam -nofpscomp -g -O3 -align all&nbsp; &nbsp;-g -extend-source -noaltparam -nofpscomp -g -O3 -align all&nbsp; -static-intel -static-libgcc&nbsp; -static-intel -static-libgcc&nbsp; -o abinit abinit-abinit.o -static-intel -static-libgcc&nbsp; ../../src/95_drive/lib95_drive.a ../../src/94_scfcv/lib94_scfcv.a ../../src/79_seqpar_mpi/lib79_seqpar_mpi.a ../../src/78_effpot/lib78_effpot.a ../../src/78_eph/lib78_eph.a ../../src/77_ddb/lib77_ddb.a ../../src/77_suscep/lib77_suscep.a ../../src/72_response/lib72_response.a ../../src/71_bse/lib71_bse.a ../../src/71_wannier/lib71_wannier.a ../../src/70_gw/lib70_gw.a ../../src/69_wfdesc/lib69_wfdesc.a ../../src/68_dmft/lib68_dmft.a&nbsp; ../../src/68_recursion/lib68_recursion.a ../../src/68_rsprc/lib68_rsprc.a&nbsp; ../../src/67_common/lib67_common.a ../../src/66_vdwxc/lib66_vdwxc.a ../../src/66_wfs/lib66_wfs.a ../../src/66_nonlocal/lib66_nonlocal.a ../../src/65_paw/lib65_paw.a&nbsp; ../../src/64_psp/lib64_psp.a ../../src/62_iowfdenpot/lib62_iowfdenpot.a ../../src/62_wvl_wfs/lib62_wvl_wfs.a ../../src/62_poisson/lib62_poisson.a ../../src/62_cg_noabirule/lib62_cg_noabirule.a ../../src/62_ctqmc/lib62_ctqmc.a ../../src/61_occeig/lib61_occeig.a ../../src/59_ionetcdf/lib59_ionetcdf.a ../../src/57_iovars/lib57_iovars.a ../../src/57_iopsp_parser/lib57_iopsp_parser.a ../../src/56_recipspace/lib56_recipspace.a ../../src/56_xc/lib56_xc.a ../../src/56_mixing/lib56_mixing.a ../../src/56_io_mpi/lib56_io_mpi.a ../../src/55_abiutil/lib55_abiutil.a ../../src/54_spacepar/lib54_spacepar.a ../../src/53_ffts/lib53_ffts.a&nbsp; ../../src/52_fft_mpi_noabirule/lib52_fft_mpi_noabirule.a ../../src/51_manage_mpi/lib51_manage_mpi.a ../../src/49_gw_toolbox_oop/lib49_gw_toolbox_oop.a ../../src/46_diago/lib46_diago.a ../../src/45_xgTools/lib45_xgTools.a ../../src/45_geomoptim/lib45_geomoptim.a ../../src/44_abitypes_defs/lib44_abitypes_defs.a ../../src/44_abitools/lib44_abitools.a ../../src/43_wvl_wrappers/lib43_wvl_wrappers.a ../../src/43_ptgroups/lib43_ptgroups.a ../../src/42_parser/lib42_parser.a ../../src/42_nlstrain/lib42_nlstrain.a ../../src/42_libpaw/lib42_libpaw.a ../../src/41_xc_lowlevel/lib41_xc_lowlevel.a ../../src/41_geometry/lib41_geometry.a ../../src/32_util/lib32_util.a ../../src/29_kpoints/lib29_kpoints.a ../../src/28_numeric_noabirule/lib28_numeric_noabirule.a ../../src/27_toolbox_oop/lib27_toolbox_oop.a ../../src/21_hashfuncs/lib21_hashfuncs.a ../../src/18_timing/lib18_timing.a ../../src/17_libtetra_ext/lib17_libtetra_ext.a ../../src/16_hideleave/lib16_hideleave.a&nbsp; ../../src/14_hidewrite/lib14_hidewrite.a ../../src/12_hide_mpi/lib12_hide_mpi.a ../../src/11_memory_mpi/lib11_memory_mpi.a ../../src/10_dumpinfo/lib10_dumpinfo.a ../../src/10_defs/lib10_defs.a ../../src/02_clib/lib02_clib.a&nbsp; -L/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/lib/intel64 -Wl,--start-group&nbsp; -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -Wl,--end-group -lpthread -lm -ldl -L/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/lib/intel64 -Wl,--start-group&nbsp; -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -Wl,--end-group -lpthread -lm -ldl -lrt -L/opt/intel//compilers_and_libraries_2019.1.144/linux/mpi/intel64/lib/release -L/opt/intel//compilers_and_libraries_2019.1.144/linux/mpi/intel64/lib -L/opt/intel/clck/2019.0/lib/intel64 -L/opt/intel//compilers_and_libraries_2019.1.144/linux/mpi/intel64/libfabric/lib -L/opt/intel/compilers_and_libraries_2019.1.144/linux/ipp/lib/intel64 -L/opt/intel/compilers_and_libraries_2019.1.144/linux/compiler/lib/intel64_lin -L/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/lib/intel64_lin -L/opt/intel/compilers_and_libraries_2019.1.144/linux/tbb/lib/intel64/gcc4.7 -L/opt/intel/compilers_and_libraries_2019.1.144/linux/daal/lib/intel64_lin -L/opt/intel/compilers_and_libraries_2019.1.144/linux/daal/../tbb/lib/intel64_lin/gcc4.4 -L/opt/intel//compilers_and_libraries_2019.1.144/linux/mpi/intel64/libfabric/lib/../lib/ -L/usr/lib/gcc/x86_64-linux-gnu/7/ -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/ -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib/ -L/lib/x86_64-linux-gnu/ -L/lib/../lib64 -L/lib/../lib/ -L/usr/lib/x86_64-linux-gnu/ -L/usr/lib/../lib/ -L/opt/intel/clck/2019.0/lib/intel64/ -L/opt/intel//compilers_and_libraries_2019.1.144/linux/mpi/intel64/libfabric/lib/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/ipp/lib/intel64/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/compiler/lib/intel64_lin/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/lib/intel64_lin/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/tbb/lib/intel64/gcc4.7/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/daal/lib/intel64_lin/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/daal/../tbb/lib/intel64_lin/gcc4.4/ -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../ -L/lib64 -L/lib/ -L/usr/lib -L/usr/lib/i386-linux-gnu -lmpifort -lmpi -ldl -lrt -lpthread -lifport -lifcoremt -limf -lsvml -lm -lipgo -lirc -lirc_s -L/opt/intel//compilers_and_libraries_2019.1.144/linux/mpi/intel64/lib/release -L/opt/intel//compilers_and_libraries_2019.1.144/linux/mpi/intel64/lib -L/opt/intel/clck/2019.0/lib/intel64 -L/opt/intel//compilers_and_libraries_2019.1.144/linux/mpi/intel64/libfabric/lib -L/opt/intel/compilers_and_libraries_2019.1.144/linux/ipp/lib/intel64 -L/opt/intel/compilers_and_libraries_2019.1.144/linux/compiler/lib/intel64_lin -L/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/lib/intel64_lin -L/opt/intel/compilers_and_libraries_2019.1.144/linux/tbb/lib/intel64/gcc4.7 -L/opt/intel/compilers_and_libraries_2019.1.144/linux/daal/lib/intel64_lin -L/opt/intel/compilers_and_libraries_2019.1.144/linux/daal/../tbb/lib/intel64_lin/gcc4.4 -L/opt/intel//compilers_and_libraries_2019.1.144/linux/mpi/intel64/libfabric/lib/../lib/ -L/usr/lib/gcc/x86_64-linux-gnu/7/ -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/ -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib/ -L/lib/x86_64-linux-gnu/ -L/lib/../lib64 -L/lib/../lib/ -L/usr/lib/x86_64-linux-gnu/ -L/usr/lib/../lib/ -L/opt/intel/clck/2019.0/lib/intel64/ -L/opt/intel//compilers_and_libraries_2019.1.144/linux/mpi/intel64/libfabric/lib/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/ipp/lib/intel64/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/compiler/lib/intel64_lin/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/lib/intel64_lin/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/tbb/lib/intel64/gcc4.7/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/daal/lib/intel64_lin/ -L/opt/intel/compilers_and_libraries_2019.1.144/linux/daal/../tbb/lib/intel64_lin/gcc4.4/ -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../ -L/lib64 -L/lib/ -L/usr/lib -L/usr/lib/i386-linux-gnu -lmpifort -lmpi -ldl -lrt -lpthread -lifport -lifcoremt -limf -lsvml -lm -lipgo -lirc -lirc_s </code></pre></div><br /><br />It appears that the mpiifort wrapper has set the libraries to link against and the with_mpi_libs specified in the config.ac file has been ignored.<br /><br />Here is what I get when I run abinit directly:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>../../../src/98_main/abinit &lt; t01.stdin &gt; OUT<br />forrtl: severe (24): end-of-file during read, unit 5, file /proc/33337/fd/0<br />Image&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; PC&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Routine&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Line&nbsp; &nbsp; &nbsp; &nbsp; Source&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br />libifcoremt.so.5&nbsp; &nbsp;00007F49529B97B6&nbsp; for__io_return&nbsp; &nbsp; &nbsp; &nbsp; Unknown&nbsp; Unknown<br />libifcoremt.so.5&nbsp; &nbsp;00007F49529F7C00&nbsp; for_read_seq_fmt&nbsp; &nbsp; &nbsp; Unknown&nbsp; Unknown<br />abinit&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;00000000018A6119&nbsp; Unknown&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Unknown&nbsp; Unknown<br />abinit&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0000000000407C49&nbsp; Unknown&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Unknown&nbsp; Unknown<br />abinit&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0000000000407942&nbsp; Unknown&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Unknown&nbsp; Unknown<br />libc-2.27.so&nbsp; &nbsp; &nbsp; &nbsp;00007F49503F1B97&nbsp; __libc_start_main&nbsp; &nbsp; &nbsp;Unknown&nbsp; Unknown<br />abinit&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;000000000040782A&nbsp; Unknown&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Unknown&nbsp; Unknown<br /><br />tail -25 OUT<br />&nbsp; ABINIT 8.10.2<br />&nbsp; <br />&nbsp; Give name for formatted input file: <br /></code></pre></div><br /><br />Here is what I get when I run abinit via mpirun:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>mpirun -np 1 ../../../src/98_main/abinit &lt; t01.stdin &gt; OUT<br /><br />tail -25 OUT<br />- Computational Materials Science 25, 478-492 (2002). http://dx.doi.org/10.1016/S0927-0256(02)00325-7<br />- Comment : the original paper describing the ABINIT project.<br />- DOI and bibtex : see https://docs.abinit.org/theory/bibliography/#gonze2002<br />&nbsp;Proc.&nbsp; &nbsp;0 individual time (sec): cpu=&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.1&nbsp; wall=&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.1<br />&nbsp;<br />&nbsp;Calculation completed.<br />.Delivered&nbsp; &nbsp;6 WARNINGs and&nbsp; &nbsp;8 COMMENTs to log file.<br /><br />--- !FinalSummary<br />program: abinit<br />version: 8.10.2<br />start_datetime: Sun Mar 10 09:32:31 2019<br />end_datetime: Sun Mar 10 09:32:31 2019<br />overall_cpu_time:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.1<br />overall_wall_time:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.1<br />exit_requested_by_user: no <br />timelimit: 0<br />pseudos: <br />&nbsp; &nbsp; H&nbsp; &nbsp;: eb3a1fb3ac49f520fd87c87e3deb9929<br />usepaw: 0<br />mpi_procs: 1<br />omp_threads: 1<br />num_warnings: 6<br />num_comments: 8<br />...<br /></code></pre></div><br /><br />So maybe this is a difference between Intel 19 and Intel 17? Unfortunately, I don't have Intel 17 installed on my system to test that.</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Sun Mar 10, 2019 8:04 pm</strong></div>
				<div class="author">by <strong>frodo</strong></div>
				<div class="content">I added -traceback to FC_FLAGS_EXTRA and now get the file and linenumber where abinit is failing:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>../../../src/98_main/abinit &lt; t01.stdin &gt; OUT-traceback<br />forrtl: severe (24): end-of-file during read, unit 5, file /proc/26824/fd/0<br />Image&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; PC&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Routine&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Line&nbsp; &nbsp; &nbsp; &nbsp; Source&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br />libifcoremt.so.5&nbsp; &nbsp;00007F0847FAC7B6&nbsp; for__io_return&nbsp; &nbsp; &nbsp; &nbsp; Unknown&nbsp; Unknown<br />libifcoremt.so.5&nbsp; &nbsp;00007F0847FEAC00&nbsp; for_read_seq_fmt&nbsp; &nbsp; &nbsp; Unknown&nbsp; Unknown<br />abinit&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;000000000187BC1F&nbsp; m_dtfil_mp_iofn1_&nbsp; &nbsp; &nbsp; &nbsp; 1363&nbsp; m_dtfil.F90<br />abinit&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0000000000407C49&nbsp; MAIN__&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 251&nbsp; abinit.F90<br />abinit&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0000000000407942&nbsp; Unknown&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Unknown&nbsp; Unknown<br />libc-2.27.so&nbsp; &nbsp; &nbsp; &nbsp;00007F08459E4B97&nbsp; __libc_start_main&nbsp; &nbsp; &nbsp;Unknown&nbsp; Unknown<br />abinit&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;000000000040782A&nbsp; Unknown&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Unknown&nbsp; Unknown</code></pre></div><br /><br />Line 1363 in m_dtfil.F90 is just a straightforward read (the preceeding write succeeds, as you can see in the OUT file I posted)<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>!&nbsp; Read name of input file (std_in):<br />&nbsp; &nbsp;write(std_out,*,err=10,iomsg=errmsg)' Give name for formatted input file: '<br />&nbsp; &nbsp;read(std_in, '(a)',err=10,iomsg=errmsg ) filnam(1)<br /></code></pre></div></div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Mon Mar 11, 2019 4:51 am</strong></div>
				<div class="author">by <strong>frodo</strong></div>
				<div class="content">I tried upgrading to from Intel Parallel Studio XE Cluster Edition 2019 Update 1 to Intel Parallel Studio XE Cluster Edition 2019 Update 3.<br /><br />Now:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>mpiifort -V<br />Intel(R) Fortran Intel(R) 64 Compiler for applications running on Intel(R) 64, Version 19.0.3.199 Build 20190206<br />Copyright (C) 1985-2019 Intel Corporation.&nbsp; All rights reserved.</code></pre></div><br /><br />I also tried compiling with enable_mpi_io=&quot;no&quot;.<br /><br />Neither change made any difference. I still get forrtl severe (24) unless I run abinit with mpirun.</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Wed Mar 13, 2019 11:51 am</strong></div>
				<div class="author">by <strong>ebousquet</strong></div>
				<div class="content">Hi Frodo,<br />So, unless somebody else can comment on that, it sounds like we don't really know what's wrong here, but since it works by calling mpirun -np 1, in the meantime just run it like that (it'll probably the same for all other exec, e.g. anaddb, etc)... Otherwise recompile another exec in sequential...<br />Best wishes,<br />Eric</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Wed May 01, 2019 10:13 pm</strong></div>
				<div class="author">by <strong>frodo</strong></div>
				<div class="content">I discovered that there are a couple of comments on the Intel Developer Forum from others who noticed similar behavior.<br /><br />Starting with Intel 19, you need to execute a program with mpirun (even for np=1) if it reads or writes stdin AFTER calling MPI_Init, otherwise the program will fail, as I noted earlier. If you do i/o to stdin BEFORE calling MPI_Init, the program succeeds.<br /><br />For example:<br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>program test_mpi_1<br />read(5,*)n<br />write(6,*)n<br />call MPI_Init(ierr)<br />call MPI_Finalize(ierr)<br />end program test_mpi_1</code></pre></div><br />can be successfully run as &quot;test_mpi_1 1&quot;<br /><br />However,<br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>program test_mpi_2<br />call MPI_Init(ierr)<br />read(5,*)n<br />write(6,*)n<br />call MPI_Finalize(ierr)<br />end program test_mpi_2</code></pre></div><br />fails if you try to run &quot;test_mpi_2 1&quot; but succeeds if you run &quot;mpirun -n 1 test_mpi_2 1&quot;.<br /><br />Abinit calls xmpi_init (a wrapper for MPI_Init) before doing i/o to stdin, so if it is compiled with Intel 19, it can't be run without mpirun. This breaks &quot;make check&quot; and requires that all tests be run with &quot;runtests.py --force-mpirun&quot;.<br /><br />I installed Intel Parallel Studio XE 2018 and recompiled abinit (with mpi enabled) and verified that it CAN be run directly (without mpirun) and does not fail when doing i/o to stdin.<br /><br />In summary, Intel 19 works fine with abinit but requires that you always use mpirun.<br /><br />Another smaller point relates to the fortran compiler flag to enable openmp code. Before 19, either fopenmp or qopenmp were accepted. As of Intel 19, only qopenmp is accepted and fopenmp is no longer recognized.<br /><br />These changes in behavior should be taken into account in future releases of abinit.</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Thu Jun 06, 2019 10:03 am</strong></div>
				<div class="author">by <strong>ebousquet</strong></div>
				<div class="content">Dear Frodo,<br />Thanks for your reply and reporting the details regarding this problem, this will help future users and developers.<br />Best wishes,<br />Eric</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Thu Jun 06, 2019 5:30 pm</strong></div>
				<div class="author">by <strong>frodo</strong></div>
				<div class="content">In addition to making the &quot;make check&quot; tests fail, this problem also makes abipy scripts that invoke abinit or anaddb fail since they expect to be able to invoke abinit or anaddb without using mpirun. Using &quot;runtests.py --force-mpirun&quot; is a simple work around for not being able to use &quot;make check&quot;. However, I couldn't see any built-in way to make the abipy scripts work.<br /><br />This was a big issue since it prevents use of the excellent abipy library. So I came up with the following simple shell script as a workaround until a future abinit release that addresses this issue is available.<br /><br />I renamed the abinit executable to &quot;abinit-mpi&quot; and created the following shell script named &quot;abinit&quot;:<br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>#!/bin/bash<br />PARENT=`ps --no-heading -o %c -p $PPID`<br />parentstr=${PARENT:0:4}<br />if &#91; ${parentstr} = &quot;hydr&quot; &#93;<br />then<br />&nbsp; &nbsp;abinit-mpi &quot;$@&quot;<br />else<br />&nbsp; &nbsp;mpirun -n 1 abinit-mpi &quot;$@&quot;<br />fi</code></pre></div><br /><br />With intel19, invoking abinit via &quot;mpirun -n m abinit ...arguments...&quot; passes the &quot;abinit&quot; command to a &quot;hydra_pmi_proxy&quot; script, which invokes it. The bash script above checks for this, and passes &quot;abinit-mpi&quot; to the hydra_pmi_proxy script along with any command line arguments (...arguments...). On the other hand, if abinit is invoked directly via &quot;abinit ...arguments...&quot;, then the bash script invokes it with &quot;mpirun -n 1 abinit-mpi ...arguments...&quot;.<br /><br />anaddb suffers from the same issue, so I wrote a similar script for invoking it.<br /><br />With these scripts, &quot;make check&quot;, and especially the abipy library, all work normally.<br /><br />Perhaps these scripts will be useful to others who run into this problem with intel19.</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Tue Jul 02, 2019 6:06 pm</strong></div>
				<div class="author">by <strong>frodo</strong></div>
				<div class="content">Update: I received confirmation from Intel that this behavior is indeed a bug. They claim it will be fixed in one of the Intel MPI 2020 updates.</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Fri Jul 12, 2019 4:37 pm</strong></div>
				<div class="author">by <strong>ebousquet</strong></div>
				<div class="content">OK, good to know!<br />Thanks a lot,<br />Eric</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0&nbsp;&nbsp;<span style="color: #006600">[SOLVED]</span></h3>
				<div class="date">Posted: <strong>Sat Nov 30, 2019 6:57 pm</strong></div>
				<div class="author">by <strong>frodo</strong></div>
				<div class="content">Hi,<br>
<br>
Intel released 2019 Update 6 for the Intel MPI library on Nov 6. I tested it and it does indeed fix the problem I reported above.<br>
<br>
Abinit executables compiled with the Intel MPI library Update 6 can now be run without mpirun.<br>
<br>
Intel has not yet released an update for the full Parallel Studio XE Suite that includes update 6 for the MPI library component, so you currently have to install the MPI update 6 library separately. It gets installed in a parallel_studio_xe_2020 directory instead of a parallel_studio_xe_2019 directory, so I suspect the next update of the full suite will be a 2020 version instead of a 2019 update release. Until the 2020 version of the full suite is released, you can include the standalone update 6 mpi library by sourcing mpivars.sh from its directory tree after sourcing psxevars.sh from the 2019 parallel studio directory tree.<br>
<br>
Cheers</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Thu Feb 27, 2020 8:17 pm</strong></div>
				<div class="author">by <strong>Olivier.5590</strong></div>
				<div class="content">And do we have an ETA for the 2020 version, or did I miss it? Thanks in advance!</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: Compiling with MPI and Intel 19.0</h3>
				<div class="date">Posted: <strong>Tue Mar 03, 2020 4:46 pm</strong></div>
				<div class="author">by <strong>jbeuken</strong></div>
				<div class="content"><a href="https://wiki.abinit.org/doku.php?id=developers:planning" class="postlink">see Planning on wiki</a></div>
			</div>
			<hr />
			</div>

	<div id="page-footer" class="page-footer">
		<div class="page-number">All times are <span title="Europe/Brussels">UTC+02:00</span><br />Page <strong>1</strong> of <strong>1</strong></div>
			<div class="copyright">
				<p>Powered by <a href="https://www.phpbb.com/">phpBB</a>&reg; Forum Software &copy; phpBB Limited
				</p>
							</div>
	</div>
</div>

</body>

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=3&t=4057&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 20 Sep 2024 18:32:37 GMT -->
</html>
