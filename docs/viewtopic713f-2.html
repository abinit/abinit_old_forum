<!DOCTYPE html>
<html dir="ltr" lang="en-gb">

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=3&t=1964&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 03:42:17 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="robots" content="noindex" />

<title>ABINIT Discussion Forums &bull; abinit7.0.4 segmentation fault - compiled with intel11.1 mpi</title>

<link href="styles/flat-style/theme/print.css" rel="stylesheet">
<link href="styles/flat-style/theme/bidi.css" rel="stylesheet">
</head>
<body id="phpbb" class="ltr">
<div id="wrap" class="wrap">
	<a id="top" class="top-anchor" accesskey="t"></a>

	<div id="page-header">
		<h1>ABINIT Discussion Forums</h1>
		<p>The meeting place for ABINIT users and developers<br /><a href="index.html">https://forum.abinit.org/</a></p>

		<h2>abinit7.0.4 segmentation fault - compiled with intel11.1 mpi</h2>
		<p><a href="viewtopicb74a-2.html?f=3&amp;t=1964">https://forum.abinit.org/viewtopic.php?f=3&amp;t=1964</a></p>
	</div>

	<div id="page-body" class="page-body">
		<div class="page-number">Page <strong>1</strong> of <strong>1</strong></div>
					<div class="post">
				<h3>abinit7.0.4 segmentation fault - compiled with intel11.1 mpi</h3>
				<div class="date">Posted: <strong>Thu Jan 24, 2013 2:02 pm</strong></div>
				<div class="author">by <strong>Mansour</strong></div>
				<div class="content">Hello<br />This is first time I am using abinit; but anyway I have compiled its 7.0.4 version on Linux 2.6.32-279.9.1.el6.x86_64 (Red Hat 4.4.6-4) with a simple mpi-prefix option as : <br /><span style="color: #0000FF">../abinit-7.0.4/configure --enable-mpi --with-mpi-prefix=/cluster/mpi/openmpi/1.6.3/intel/</span><br />Of course I loaded related modules before: <br /><span style="color: #0000FF">Currently Loaded Modulefiles:<br />  1) intel/11.1046             2) mpi/intel/openmpi/1.6.3</span><br />there were some warnings but it could finish configuration finally. After that, by make -j4 final step was done.<br />Unfortunately, when I try to test it by make tests_in or tests_min lots of segmentation errors came out. I also submit it on cluster with 8 cpus on an nod (for tbasepar_1test case) but same problem was occurred (i.e. segmentation fault).<br />The last part of its log file is like: <br /><br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>&nbsp;pspatm: atomic psp has been read&nbsp; and splines computed<br /><br />&nbsp; &nbsp;2.39408461E+03&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ecore*ucvol(ha*bohr**3)<br />&nbsp;symatm: atom number&nbsp; &nbsp; &nbsp;1 is reached starting at atom<br />&nbsp; &nbsp;1&nbsp; 3&nbsp; 2&nbsp; 4&nbsp; 1&nbsp; 3&nbsp; 2&nbsp; 4&nbsp; 1&nbsp; 3&nbsp; 2&nbsp; 4&nbsp; 1&nbsp; 3&nbsp; 2&nbsp; 4&nbsp; 1&nbsp; 3&nbsp; 2&nbsp; 4&nbsp; 1&nbsp; 3&nbsp; 2&nbsp; 4<br />&nbsp;symatm: atom number&nbsp; &nbsp; &nbsp;2 is reached starting at atom<br />&nbsp; &nbsp;2&nbsp; 4&nbsp; 1&nbsp; 3&nbsp; 2&nbsp; 4&nbsp; 1&nbsp; 3&nbsp; 4&nbsp; 2&nbsp; 3&nbsp; 1&nbsp; 3&nbsp; 1&nbsp; 4&nbsp; 2&nbsp; 3&nbsp; 1&nbsp; 4&nbsp; 2&nbsp; 4&nbsp; 2&nbsp; 3&nbsp; 1<br />&nbsp;symatm: atom number&nbsp; &nbsp; &nbsp;3 is reached starting at atom<br />&nbsp; &nbsp;3&nbsp; 1&nbsp; 4&nbsp; 2&nbsp; 4&nbsp; 2&nbsp; 3&nbsp; 1&nbsp; 2&nbsp; 4&nbsp; 1&nbsp; 3&nbsp; 2&nbsp; 4&nbsp; 1&nbsp; 3&nbsp; 4&nbsp; 2&nbsp; 3&nbsp; 1&nbsp; 3&nbsp; 1&nbsp; 4&nbsp; 2<br />&nbsp;symatm: atom number&nbsp; &nbsp; &nbsp;4 is reached starting at atom<br />&nbsp; &nbsp;4&nbsp; 2&nbsp; 3&nbsp; 1&nbsp; 3&nbsp; 1&nbsp; 4&nbsp; 2&nbsp; 3&nbsp; 1&nbsp; 4&nbsp; 2&nbsp; 4&nbsp; 2&nbsp; 3&nbsp; 1&nbsp; 2&nbsp; 4&nbsp; 1&nbsp; 3&nbsp; 2&nbsp; 4&nbsp; 1&nbsp; 3<br />&nbsp;newkpt: spin channel isppol2 =&nbsp; &nbsp; &nbsp;1<br />-P-0000&nbsp; wfconv:&nbsp; &nbsp; 40 bands initialized randomly with npw=&nbsp; &nbsp;675, for ikpt=&nbsp; &nbsp; &nbsp;1<br />&nbsp;newkpt: spin channel isppol2 =&nbsp; &nbsp; &nbsp;2<br />&nbsp;newkpt: loop on k-points done in parallel<br />-P-0000&nbsp; leave_test : synchronization done...<br />&nbsp;pareigocc : MPI_ALLREDUCE<br /><br />&nbsp;setup2: Arith. and geom. avg. npw (full set) are&nbsp; &nbsp; &nbsp;675.000&nbsp; &nbsp; &nbsp;675.000<br />&nbsp;initro : for itypat=&nbsp; 1, take decay length=&nbsp; &nbsp; &nbsp; 0.8000,<br />&nbsp;initro : indeed, coreel=&nbsp; &nbsp; &nbsp;18.0000, nval=&nbsp; 8 and densty=&nbsp; 0.0000E+00.<br /><br />================================================================================<br /><br />&nbsp;getcut: wavevector=&nbsp; 0.0000&nbsp; 0.0000&nbsp; 0.0000&nbsp; ngfft=&nbsp; 36&nbsp; 36&nbsp; 36<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ecut(hartree)=&nbsp; &nbsp; &nbsp;30.000&nbsp; &nbsp;=&gt; boxcut(ratio)=&nbsp; &nbsp;2.08583<br /><br />&nbsp;ewald : nr and ng are&nbsp; &nbsp; 3 and&nbsp; &nbsp;15<br /><br />&nbsp;ITER STEP NUMBER&nbsp; &nbsp; &nbsp;1<br />&nbsp;vtorho : nnsclo_now=&nbsp; 2, note that nnsclo,dbl_nnsclo,istep=&nbsp; 0 0&nbsp; 1<br />&nbsp; ****&nbsp; In vtorho for isppol=&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1<br />&nbsp; ****&nbsp; In vtorho for isppol=&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2<br />--------------------------------------------------------------------------<br />mpirun noticed that process rank 1 with PID 25632 on node node156.cluster.icams.ruhr-uni-bochum.de exited on signal 11 (Segmentation fault).<br />--------------------------------------------------------------------------<br /></code></pre></div><br /> I read other posts especially this <a href="viewtopic329f.html?f=3&amp;t=1514&amp;start=25" class="postlink">http://forum.abinit.org/viewtopic.php?f=3&amp;t=1514&amp;start=25</a> but I couldn't realize the problem yet. While the discussed OS was different in that post but I guess in my case also a wrong mixture of different libraries was chosen.<br />I attached config.log file which includes required information about my platform and probable wrong options. <br /><br />Thanks in advance for your helps.<br />Mansour</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: abinit7.0.4 segmentation fault - compiled with intel11.1</h3>
				<div class="date">Posted: <strong>Thu Jan 24, 2013 3:58 pm</strong></div>
				<div class="author">by <strong>Alain_Jacques</strong></div>
				<div class="content">Hi Mansour,<br /><br />I suspect that your abinit has been linked to a flaky linear algebra library as configure Russian roulette seems to find one apparently suitable to replace the fallback NETLIB. As a consequence the binary crashes at the first call of a dot product function - you can check this with a run of gdb.<br />Anyway there are several ways to circumvent this by trying either:<br /><br />* to configure with --with-linalg-flavor=&quot;none&quot;, recompile and test with a make tests_v1 - this is the safest choice but there is a speed penalty.<br />* or to configure with --enable-zdot-bugfix=&quot;yes&quot;, recompile and re-check (may use your enhanced lib but wannier90 is probably broken)<br />* or use MKL libraries for BLAS, LAPACK and FFT, probably the most efficient choice.<br /><br />Kind regards,<br /><br />Alain</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: abinit7.0.4 segmentation fault - compiled with intel11.1</h3>
				<div class="date">Posted: <strong>Fri Jan 25, 2013 12:05 pm</strong></div>
				<div class="author">by <strong>Mansour</strong></div>
				<div class="content">Dear Alain<br /><br />Thank you very much for your quick reply. Yes, it does work. In fact by that linalg-flavor flag it set the NETLIB for linear algebra package correctly.<br /><span style="color: #0000BF">../abinit-7.0.4/configure --enable-mpi=yes --enable-optim=yes --with-mpi-prefix=/cluster/mpi/openmpi/1.6.3/intel/ --with-linalg\<br />-flavor=none</span><br /><br /> But I am suspected whether its version is proper for mpi calculation or not. As I saw during <span style="color: #0000BF">make</span> command the <blockquote class="uncited"><div>sequential version of linear algebra package</div></blockquote> has been chosen. Dose it really matter for parallel calculation? (new config.log is attached)<br /><br />I also run <em class="text-italics">tests_v1</em> successfully. The tutorial first test case <em class="text-italics">tbasepar_1</em> was also fine with different number of cpus but I was confused with results for second test case <em class="text-italics">tbasepar_2</em> which is considered spin calculation. In fact its results is depending on number of cpus!. I checked with different number of cpus (1,8 and 16); the first one's result was quite same as reference output file but for two next cases the calculated energy and stress tensor components have significant  deviation from reference value (+-10 times higher). While results for 8 and 16 cpus are the same. The question is that, treating this case with larger number of cpus causes the problem (because it has too few K-points) or is there any mistake related to code compiling?<br /><br />The comparison by sdiff command between 8-cpu and reference output files is shown here: (both are attached also)<br /><div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>-------------------------------------------------------------&nbsp; &nbsp;-------------------------------------------------------------<br />&nbsp;Components of total free energy (in Hartree) :&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Components of total free energy (in Hartree) :<br /><br />&nbsp; &nbsp; Kinetic energy&nbsp; =&nbsp; 1.57704767360493E+02&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;Kinetic energy&nbsp; =&nbsp; 1.25650794248530E+02<br />&nbsp; &nbsp; Hartree energy&nbsp; =&nbsp; 1.82744721367767E+01&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;Hartree energy&nbsp; =&nbsp; 1.44063774642969E+01<br />&nbsp; &nbsp; XC energy&nbsp; &nbsp; &nbsp; &nbsp;= -2.01317375494705E+01&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;XC energy&nbsp; &nbsp; &nbsp; &nbsp;= -1.70626672615728E+01<br />&nbsp; &nbsp; Ewald energy&nbsp; &nbsp; = -8.36499227815283E+01&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Ewald energy&nbsp; &nbsp; = -8.36499227815283E+01<br />&nbsp; &nbsp; PspCore energy&nbsp; =&nbsp; 6.97983851565236E+00&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;PspCore energy&nbsp; =&nbsp; 6.97983851565236E+00<br />&nbsp; &nbsp; Loc. psp. energy= -5.37285622189003E+01&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;Loc. psp. energy= -4.83382491267464E+01<br />&nbsp; &nbsp; NL&nbsp; &nbsp;psp&nbsp; energy= -1.00332543896596E+02&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;NL&nbsp; &nbsp;psp&nbsp; energy= -7.79796526379097E+01<br />&nbsp; &nbsp; &gt;&gt;&gt;&gt;&gt; Internal E= -7.48836884335736E+01&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;&gt;&gt;&gt;&gt;&gt; Internal E= -7.99934815792781E+01<br /><br />&nbsp; &nbsp; -kT*entropy&nbsp; &nbsp; &nbsp;= -2.32062757842293E-02&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;-kT*entropy&nbsp; &nbsp; &nbsp;= -9.13291316865402E-02<br />&nbsp; &nbsp; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Etotal= -7.49068947093578E+01&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Etotal= -8.00848107109647E+01<br /><br />&nbsp;Other information on the energy :&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Other information on the energy :<br />&nbsp; &nbsp; Total energy(eV)= -2.03832026569694E+03 ; Band energy (Ha |&nbsp; &nbsp; &nbsp;Total energy(eV)= -2.17921852561150E+03 ; Band energy (Ha<br />-------------------------------------------------------------&nbsp; &nbsp;-------------------------------------------------------------<br /><br />&nbsp;Cartesian components of stress tensor (hartree/bohr^3)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Cartesian components of stress tensor (hartree/bohr^3)<br />&nbsp; sigma(1 1)= -9.13362800E-03&nbsp; sigma(3 2)=&nbsp; 0.00000000E+00&nbsp; &nbsp; |&nbsp; &nbsp;sigma(1 1)=&nbsp; 2.37971568E-02&nbsp; sigma(3 2)=&nbsp; 0.00000000E+00<br />&nbsp; sigma(2 2)= -9.13362800E-03&nbsp; sigma(3 1)=&nbsp; 0.00000000E+00&nbsp; &nbsp; |&nbsp; &nbsp;sigma(2 2)=&nbsp; 2.37971568E-02&nbsp; sigma(3 1)=&nbsp; 0.00000000E+00<br />&nbsp; sigma(3 3)= -9.13362800E-03&nbsp; sigma(2 1)=&nbsp; 0.00000000E+00&nbsp; &nbsp; |&nbsp; &nbsp;sigma(3 3)=&nbsp; 2.37971568E-02&nbsp; sigma(2 1)=&nbsp; 0.00000000E+00<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br />-Cartesian components of stress tensor (GPa)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#91;Pressur | -Cartesian components of stress tensor (GPa)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#91;Pressur<br />- sigma(1 1)= -2.68720568E+02&nbsp; sigma(3 2)=&nbsp; 0.00000000E+00&nbsp; &nbsp; | - sigma(1 1)=&nbsp; 7.00136408E+02&nbsp; sigma(3 2)=&nbsp; 0.00000000E+00<br />- sigma(2 2)= -2.68720568E+02&nbsp; sigma(3 1)=&nbsp; 0.00000000E+00&nbsp; &nbsp; | - sigma(2 2)=&nbsp; 7.00136408E+02&nbsp; sigma(3 1)=&nbsp; 0.00000000E+00<br />- sigma(3 3)= -2.68720568E+02&nbsp; sigma(2 1)=&nbsp; 0.00000000E+00&nbsp; &nbsp; | - sigma(3 3)=&nbsp; 7.00136408E+02&nbsp; sigma(2 1)=&nbsp; 0.00000000E+00<br /><br />== END DATASET(S) ===========================================&nbsp; &nbsp;== END DATASET(S) ===========================================<br />=============================================================&nbsp; &nbsp;=============================================================<br /><br />&nbsp;-outvars: echo values of variables after computation&nbsp; ------&nbsp; &nbsp; -outvars: echo values of variables after computation&nbsp; ------<br />&nbsp; &nbsp; &nbsp; &nbsp; accesswff&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; acell&nbsp; &nbsp; &nbsp; 7.0000000000E+00&nbsp; 7.0000000000E+00&nbsp; 7.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;acell&nbsp; &nbsp; &nbsp; 7.0000000000E+00&nbsp; 7.0000000000E+00&nbsp; 7.<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; amu&nbsp; &nbsp; &nbsp; 5.58470000E+01&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;amu&nbsp; &nbsp; &nbsp; 5.58470000E+01<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;bandpp&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ecut&nbsp; &nbsp; &nbsp; 3.00000000E+01 Hartree&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ecut&nbsp; &nbsp; &nbsp; 3.00000000E+01 Hartree<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;etotal&nbsp; &nbsp; &nbsp;-7.4906894709E+01&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; etotal&nbsp; &nbsp; &nbsp;-8.0084810711E+01<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; fcart&nbsp; &nbsp; &nbsp;-1.4955468720E-01 -1.4955468720E-01 -1. |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;fcart&nbsp; &nbsp; &nbsp;-1.4955719002E-02 -1.4955719002E-02 -1.<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1.4955468720E-01&nbsp; 1.4955468720E-01 -1. |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1.4955719002E-02&nbsp; 1.4955719002E-02 -1.<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1.4955468720E-01 -1.4955468720E-01&nbsp; 1. |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1.4955719002E-02 -1.4955719002E-02&nbsp; 1.<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; -1.4955468720E-01&nbsp; 1.4955468720E-01&nbsp; 1. |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-1.4955719002E-02&nbsp; 1.4955719002E-02&nbsp; 1.<br /></code></pre></div><br /><br />Regarding your comment to use <em class="text-italics">mkl</em> libraries I checked and found its proper <em class="text-italics">BLAS</em> and <em class="text-italics">LAPACK</em> libraries for <em class="text-italics">intel</em> but not for <em class="text-italics">fftw</em>; instead, <em class="text-italics">fftw 3.2.2-intel </em>was separately installed.<br />could I use <em class="text-italics">BLAS</em> and <em class="text-italics">LAPACK</em> from <em class="text-italics">mkl </em>and this <em class="text-italics">fftw</em> ? Also, dose it better (more efficient) than current <em class="text-italics">NETLIB</em> I have?<br /><br />And final question, same procedure should I use for <span style="color: #0000BF">ABINIT7.0.5</span>?<br /><br />Best Regards<br /><br />Mansour</div>
			</div>
			<hr />
					<div class="post">
				<h3>Re: abinit7.0.4 segmentation fault - compiled with intel11.1</h3>
				<div class="date">Posted: <strong>Fri Jan 25, 2013 6:43 pm</strong></div>
				<div class="author">by <strong>Alain_Jacques</strong></div>
				<div class="content">Dear Mansour,<br /><br />When I perform parallel calculations using MPI, I generally switch to sequential linear algebra in order to avoid CPU overloading. With multithreaded blas/lapack, if you start a job on a 4 cores node by specifying mpirun -np 4 ... and each of these MPI slots open concurrent threads, you end up with more than one task per core which is inefficient due to context switching. It is difficult to control two different parallelization technologies ignoring each other. So if a MPI program has been compiled with multithreaded MKL libs, I use the info on <a href="http://software.intel.com/en-us/articles/recommended-settings-for-calling-intelr-mkl-routines-from-multi-threaded-applications" class="postlink">http://software.intel.com/en-us/articles/recommended-settings-for-calling-intelr-mkl-routines-from-multi-threaded-applications</a> to switch to sequential MKL i.e. export MKL_NUM_THREADS=1. If you're cautious when treating a problem with poor MPI parallelization but a large amount of data that may benefit from faster blas/lapack, you may keep multithreading but you have to be sure that (mpirun -np) x (MKL_NUM_THREADS=) &lt;= allocated cores.<br /><br />Now concerning the difference between 1, 2, 8, ... 16 cpus on tbasepar_2 ... You have to look at converged results. I see in the input file nstep=5 ... no way to reach convergence with such a small number of SCF loops. Retry with nstep=200 (maybe even more) and compare the results obtained with a sequential run and a parallel run on 2 processes (for spin up/down) ; they should match but be sure that both runs are converged. To use more cpus, also increase ngktp and again do comparisons between converged outputs.<br /><br />MKL provides BLAS, LAPACK and fast fourier transform and is arguably the most efficient library. As an example, I use the following extra options for a production abinit ...<div class="codebox"><p>Code: <a href="#" onclick="selectCode(this); return false;">Select all</a></p><pre><code>--enable-debug=&quot;no&quot;<br />--enable-optim=&quot;aggressive&quot;<br />--with-fft-flavor=&quot;fftw3&quot;<br />--with-fft-libs=&quot;-L/opt/intel/Compiler/11.1/073/mkl/lib/em64t -Wl,--start-group -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -Wl,--end-group -lpthread&quot;<br />--with-linalg-flavor=&quot;mkl&quot;<br />--with-linalg-libs=&quot;-L/opt/intel/Compiler/11.1/073/mkl/lib/em64t -Wl,--start-group -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -Wl,--end-group -lpthread&quot;<br /></code></pre></div><br />Adapt to your paths, make and test (yeah, fftw3 flavor for MKL fft). If you experience the same segfaults, add --enable-zdot-bugfix=&quot;yes&quot; and disable wannier90.<br /><br />Kind regards,<br /><br />Alain</div>
			</div>
			<hr />
			</div>

	<div id="page-footer" class="page-footer">
		<div class="page-number">All times are <span title="Europe/Brussels">UTC+02:00</span><br />Page <strong>1</strong> of <strong>1</strong></div>
			<div class="copyright">
				<p>Powered by <a href="https://www.phpbb.com/">phpBB</a>&reg; Forum Software &copy; phpBB Limited
				</p>
							</div>
	</div>
</div>

</body>

<!-- Mirrored from forum.abinit.org/viewtopic.php?f=3&t=1964&view=print by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 21 Sep 2024 03:42:17 GMT -->
</html>
